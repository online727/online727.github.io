<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Linear Models on Haohan's Blog</title><link>https://online727.github.io/posts/linear_models/</link><description>Recent content in Linear Models on Haohan's Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 17 Nov 2024 14:45:25 +0800</lastBuildDate><atom:link href="https://online727.github.io/posts/linear_models/index.xml" rel="self" type="application/rss+xml"/><item><title>Linear Regression</title><link>https://online727.github.io/posts/linear_models/lr/</link><pubDate>Sun, 17 Nov 2024 14:45:25 +0800</pubDate><guid>https://online727.github.io/posts/linear_models/lr/</guid><description>&lt;h2 id="01-general-expression">0.1 General Expression&lt;/h2>
&lt;p>$$y_{i}=\beta_{0}+\beta_{1}\times x_{i1}+\cdots+\beta_{p}\times x_{ip}+\epsilon_{i},\quad i=1,2,\cdots,n$$
$$
\begin{align*}
\mathbf{y}&amp;amp;=(y_{1},y_{2},\cdots,y_{n})^{T} \cr
\mathbf{X}&amp;amp;=\begin{bmatrix}1 &amp;amp; x_{11} &amp;amp; x_{12} &amp;amp; \cdots &amp;amp; x_{1p} \cr 1 &amp;amp; x_{21} &amp;amp; x_{22} &amp;amp; \cdots &amp;amp; x_{2p} \cr \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots &amp;amp; \vdots \cr 1 &amp;amp; x_{n1} &amp;amp; x_{n2} &amp;amp; \cdots &amp;amp; x_{np} \end{bmatrix} \cr
\mathbf{\beta}&amp;amp;=(\beta_{0},\beta_{1},\cdots,\beta_{p})^{T} \cr
\mathbf{\epsilon}&amp;amp;=(\epsilon_{1}, \epsilon_{2},\cdots,\epsilon_{n})^{T}
\end{align*}
$$&lt;/p>
&lt;h2 id="02-ols-assumptions">0.2 OLS Assumptions&lt;/h2>
&lt;ul>
&lt;li>The regression model is parametric linear.&lt;/li>
&lt;li>${x_{i1},x_{i2},\cdots,x_{ip}}$ are nonstochastic variables.&lt;/li>
&lt;li>$E(\epsilon_{i})=0$.&lt;/li>
&lt;li>$Var(\epsilon_{i})=\sigma^{2}$.&lt;/li>
&lt;li>${\epsilon_{i}}$ are independent random variables, so as to say: no autocorrelation, $cov(\epsilon_{i},\epsilon_{j})=0,i\neq j$.&lt;/li>
&lt;li>The regression model is set correctly, without setting bias.&lt;/li>
&lt;/ul>
&lt;h2 id="03-ols-estimators">0.3 OLS Estimators&lt;/h2>
&lt;h3 id="031--estimators-of-hatbeta">0.3.1 Estimators of $\hat{\beta}$&lt;/h3>
&lt;p>Formally, the OLS estimator of $\beta$ is defined by the minimizer of the &lt;strong>residual sum of squares (RSS)&lt;/strong>:
$$\hat{\mathbf{\beta}}=arg\ min_{\beta}\ S(\mathbf{\beta})$$
$$S(\mathbf{\beta})=(\mathbf{y}-\mathbf{X\beta})^{T}(\mathbf{y}-\mathbf{X\beta})=\sum\limits_{i=1}^{n}(y_{i}-\beta_{0}-\beta_{1}\times x_{i1}-\cdots-\beta_{p}\times x_{ip})^{2}$$
Derive it we can get:
$$\hat{\mathbf{\beta}}=(\mathbf{X^{T}X})^{-1}\mathbf{X^{T}y}$$&lt;/p></description></item><item><title>Questions of Coefficients</title><link>https://online727.github.io/posts/linear_models/lr_quant/lr_coefficients/</link><pubDate>Sat, 16 Nov 2024 11:45:25 +0800</pubDate><guid>https://online727.github.io/posts/linear_models/lr_quant/lr_coefficients/</guid><description>&lt;h3 id="01-product-of-beta">0.1 Product of $\beta$&lt;/h3>
&lt;p>Denote $\beta_1$ as the least squares optimal solution of $y=\beta x+\epsilon$, $\beta_2$ as the least squares optimal solution of $x=\beta y+\epsilon$. Find the min and max values of $\beta_1\beta_2$.
$$
\beta_1 = \frac{Cov(X,Y)}{Var(X)},\quad \beta_2 = \frac{Cov(X,Y)}{Var(Y)}\Rightarrow \beta_1\beta_2 = \rho_{XY}^2 \in [-1,1]
$$&lt;/p>
&lt;h3 id="02-load-memory">0.2 Load Memory&lt;/h3>
&lt;p>When performing linear regression, if the dataset is too large to fit into memory at once, how can this issue be resolved?&lt;/p></description></item><item><title>Questions of Conceptions</title><link>https://online727.github.io/posts/linear_models/lr_quant/lr_conceptions/</link><pubDate>Sat, 16 Nov 2024 11:45:25 +0800</pubDate><guid>https://online727.github.io/posts/linear_models/lr_quant/lr_conceptions/</guid><description>&lt;p>With reference to &lt;a href="https://zhuanlan.zhihu.com/p/443658898" target="_blank" rel="noopener">donggua&lt;/a>.&lt;/p>
&lt;p>I complete the answers of these questions.&lt;/p>
&lt;h2 id="01-notations">0.1 Notations&lt;/h2>
&lt;ol>
&lt;li>$y\sim(1,\boldsymbol{x})$, regress $y$ on $x$ with intercept.&lt;/li>
&lt;li>$y\sim(\boldsymbol{x})$, regress $y$ on $x$ without intercept.&lt;/li>
&lt;li>In the context of Statistics, SSE (Sum of Squares due to Error) and SSR (Sum of Squares due to Regression) are used more frequently. But in Economitrics, ESS (Explained Sum of Squares) and RSS (Residual Sum of Squares) are prefered.&lt;/li>
&lt;/ol>
&lt;h2 id="02-conceptions-and-basic-definitions">0.2 Conceptions and Basic Definitions&lt;/h2>
&lt;h5 id="021-the-assumptions-of-lr">0.2.1. The assumptions of LR&lt;/h5>
&lt;p>&lt;strong>Gauss-Markov Theory&lt;/strong>: Under the assumptions of classical linear regression, the ordinary least squares (OLS) estimator is the linear unviased estimator with the minimum variance. &lt;strong>(BLUE)&lt;/strong>&lt;/p></description></item></channel></rss>