<!doctype html><html lang=en><head><title>Linear Regression</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=/application.d60808555b3afbcfaf18c23a60e08a98667faa5a60879f6703a355b38eafb832.css integrity="sha256-1ggIVVs6+8+vGMI6YOCKmGZ/qlpgh59nA6NVs46vuDI="><link rel=icon type=image/png href=/images/site/favicon_hu8414222332455362891.png><meta property="og:url" content="https://online727.github.io/posts/linear_models/lr/"><meta property="og:site_name" content="Haohan's Blog"><meta property="og:title" content="Linear Regression"><meta property="og:description" content="Linear Regression"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-11-17T14:45:25+08:00"><meta property="article:modified_time" content="2024-11-17T14:45:25+08:00"><meta property="article:tag" content="Linear Model"><meta property="article:tag" content="Linear Regression"><meta name=twitter:card content="summary"><meta name=twitter:title content="Linear Regression"><meta name=twitter:description content="Linear Regression"><meta name=description content="Linear Regression"><script>theme=localStorage.getItem("theme-scheme")||localStorage.getItem("darkmode:color-scheme")||"light",theme=="system"&&(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?theme="dark":theme="light"),document.documentElement.setAttribute("data-theme",theme)</script></head><body class="type-posts kind-page" data-bs-spy=scroll data-bs-target=#TableOfContents data-bs-offset=80><div class="container-fluid bg-secondary wrapper"><nav class="navbar navbar-expand-xl top-navbar shadow" id=top-navbar><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button>
<i data-feather=sidebar></i>
</button>
<a class=navbar-brand href=/><img src=/images/site/main-logo_hu10708377409321002774.png id=logo alt=Logo>
Haohan's Blog</a>
<button class="navbar-toggler navbar-light" id=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#top-nav-items aria-label=menu>
<i data-feather=menu></i></button><div class="collapse navbar-collapse dynamic-navbar" id=top-nav-items><ul class="nav navbar-nav ms-auto"><li class=nav-item><a class=nav-link href=/#home>Home</a></li><li class=nav-item><a class=nav-link href=/#about>About</a></li><li class=nav-item><a class=nav-link href=/#education>Education</a></li><li class=nav-item><a class=nav-link href=/#experiences>Experiences</a></li><li class=nav-item><a class=nav-link href=/#projects>Projects</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>More</a><div class=dropdown-menu aria-labelledby=navbarDropdown><a class=dropdown-item href=/#skills>Skills</a>
<a class=dropdown-item href=/#featured-posts>Featured Posts</a>
<a class=dropdown-item href=/#recent-posts>Recent Posts</a>
<a class=dropdown-item href=/#accomplishments>Accomplishments</a></div></li><div id=top-navbar-divider></div><li class=nav-item><a class=nav-link id=blog-link href=/posts>Posts</a></li><li class=nav-item><a class=nav-link id=note-link href=/notes>Notes</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=languageSelector role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false><span class="fi fi-gb"></span>
English</a><div class=dropdown-menu aria-labelledby=languageSelector><a class="dropdown-item nav-link languages-item" href=/posts/linear_models/lr><span class="fi fi-gb"></span>
English
</a><a class="dropdown-item nav-link languages-item" href=/cn/posts/linear_models/lr><span class="fi fi-cn"></span>
简体中文</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg class=theme-icon src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme"></a><div id=themeMenu class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# data-scheme=light><img class=theme-icon src=/icons/sun-svgrepo-com.svg width=20 alt="Light Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=dark><img class=theme-icon src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=system><img class=theme-icon src=/icons/computer-svgrepo-com.svg width=20 alt="System Theme"></a></div></li></ul></div></div><img src=/images/site/main-logo_hu10708377409321002774.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu8414222332455362891.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/search><input type=text name=keyword placeholder=Search data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/posts/ data-filter=all>Posts</a></li><div class=subtree><li><i data-feather=plus-circle></i><a class=list-link href=/posts/example/> Examples</a><ul><li><a class=list-link href=/posts/example/introduction/ title=Introduction>Introduction</a></li><li><i data-feather=plus-circle></i><a class=list-link href=/posts/example/category/> Category</a><ul><li><i data-feather=plus-circle></i><a class=list-link href=/posts/example/category/sub-category/> Sub-Category</a><ul><li><a class=list-link href=/posts/example/category/sub-category/rich-content/ title="Rich Content">Rich Content</a></li></ul></li></ul></li><li><a class=list-link href=/posts/example/markdown-sample/ title="Markdown Sample">Markdown Sample</a></li><li><a class=list-link href=/posts/example/shortcodes/ title="Shortcodes Sample">Shortcodes Sample</a></li></ul></li><li><i data-feather=plus-circle></i><a class=list-link href=/posts/dltorch/> Deep Learning (Pytorch)</a><ul><li><a class=list-link href=/posts/dltorch/ch2/ title="Chapter 2 Preparation">Chapter 2 Preparation</a></li><li><a class=list-link href=/posts/dltorch/ch3/ title="Chapter 3 Linear Regression">Chapter 3 Linear Regression</a></li><li><a class=list-link href=/posts/dltorch/ch4/ title="Chapter 4 Multilayer Perceptrons">Chapter 4 Multilayer Perceptrons</a></li></ul></li><li><i data-feather=plus-circle></i><a class=list-link href=/posts/quant/> Quant</a><ul><li><i data-feather=plus-circle></i><a class=list-link href=/posts/quant/multi-factors/> Multi-Factors Model</a><ul><li><a class=list-link href=/posts/quant/multi-factors/perf-attri/ title="Performance Attribution">Performance Attribution</a></li></ul></li></ul></li><li><i data-feather=minus-circle></i><a class="active list-link" href=/posts/linear_models/> Linear Models</a><ul class=active><li><a class="active list-link" href=/posts/linear_models/lr/ title="Linear Regression">Linear Regression</a></li></ul></li><li><i data-feather=plus-circle></i><a class=list-link href=/posts/matrix_cookbook/> Matrix Cookbook</a><ul><li><i data-feather=plus-circle></i><a class=list-link href=/posts/matrix_cookbook/basic/> Matrix Basic</a><ul><li><a class=list-link href=/posts/matrix_cookbook/basic/notations/ title=Notations>Notations</a></li><li><a class=list-link href=/posts/matrix_cookbook/basic/basic_content/ title=Basics>Basics</a></li></ul></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/images/sky.jpg)></div><div class=page-content><div class="author-profile ms-auto align-self-lg-center"><img class=rounded-circle src=/images/author/zhh_hu2277249603137709564.png alt="Author Image"><h5 class=author-name>Haohan Zhao</h5><p class=text-muted>Sunday, November 17, 2024 | 5 minutes</p></div><div class=title><h1>Linear Regression</h1></div><div class=tags><ul style=padding-left:0><li class=rounded><a href=/tags/linear-model/ class="btn btn-sm btn-info">Linear Model</a></li><li class=rounded><a href=/tags/linear-regression/ class="btn btn-sm btn-info">Linear Regression</a></li></ul></div><div class=post-content id=post-content><h2 id=01-general-expression>0.1 General Expression</h2><p>$$y_{i}=\beta_{0}+\beta_{1}\times x_{i1}+\cdots+\beta_{p}\times x_{ip}+\epsilon_{i},\quad i=1,2,\cdots,n$$
$$
\begin{align*}
\mathbf{y}&=(y_{1},y_{2},\cdots,y_{n})^{T} \cr
\mathbf{X}&=\begin{bmatrix}1 & x_{11} & x_{12} & \cdots & x_{1p} \cr 1 & x_{21} & x_{22} & \cdots & x_{2p} \cr \vdots & \vdots & \vdots & \vdots & \vdots \cr 1 & x_{n1} & x_{n2} & \cdots & x_{np} \end{bmatrix} \cr
\mathbf{\beta}&=(\beta_{0},\beta_{1},\cdots,\beta_{p})^{T} \cr
\mathbf{\epsilon}&=(\epsilon_{1}, \epsilon_{2},\cdots,\epsilon_{n})^{T}
\end{align*}
$$</p><h2 id=02-ols-assumptions>0.2 OLS Assumptions</h2><ul><li>The regression model is parametric linear.</li><li>${x_{i1},x_{i2},\cdots,x_{ip}}$ are nonstochastic variables.</li><li>$E(\epsilon_{i})=0$.</li><li>$Var(\epsilon_{i})=\sigma^{2}$.</li><li>${\epsilon_{i}}$ are independent random variables, so as to say: no autocorrelation, $cov(\epsilon_{i},\epsilon_{j})=0,i\neq j$.</li><li>The regression model is set correctly, without setting bias.</li></ul><h2 id=03-ols-estimators>0.3 OLS Estimators</h2><h3 id=031--estimators-of-hatbeta>0.3.1 Estimators of $\hat{\beta}$</h3><p>Formally, the OLS estimator of $\beta$ is defined by the minimizer of the <strong>residual sum of squares (RSS)</strong>:
$$\hat{\mathbf{\beta}}=arg\ min_{\beta}\ S(\mathbf{\beta})$$
$$S(\mathbf{\beta})=(\mathbf{y}-\mathbf{X\beta})^{T}(\mathbf{y}-\mathbf{X\beta})=\sum\limits_{i=1}^{n}(y_{i}-\beta_{0}-\beta_{1}\times x_{i1}-\cdots-\beta_{p}\times x_{ip})^{2}$$
Derive it we can get:
$$\hat{\mathbf{\beta}}=(\mathbf{X^{T}X})^{-1}\mathbf{X^{T}y}$$</p><h3 id=032--properties-of-ols-estimators>0.3.2 Properties of OLS estimators:</h3><ul><li><strong>Linearity</strong>: the OLS estimators are linear estimators (linear functions of $\mathbf{y}$);</li><li><strong>Unbiasedness</strong>: $E(\hat{\mathbf{\beta}})=\mathbf{\beta}$;</li><li><strong>Consistent</strong>: $\hat{\mathbf{\beta}}\mathop{\rightarrow}\limits^{P}\mathbf{\beta}$ as $n\rightarrow \infty$;</li><li>The OLS estimators are <strong>Best Linear Unbiased Estimators (BLUEs)</strong>: they have smallest variance among all linear unbiased estimators (<strong>Gauss-Markov Theorem</strong>).</li></ul><h3 id=033--estimators-of-sigma2>0.3.3 Estimators of $\sigma^{2}$</h3><p>An unbiased estimator of $\sigma^{2}$ is the <strong>residual mean squared error (MSE)</strong>, which is defined as:
$$
\begin{align*}
s^{2} &= \frac{1}{n-(p+1)}\sum\limits_{i=1}^{n}e_{i}^{2} \cr
&= \frac{1}{n-(p+1)}\sum\limits_{i=1}^{n}(y_{i}-\hat{y_{i}})^{2} \cr
&= \frac{1}{n-(p+1)}(\mathbf{y-\hat{y}})^{T}(\mathbf{y-\hat{y}}) \cr
&= \frac{1}{n-(p+1)}(\mathbf{y-X\hat{\beta}})^{T}(\mathbf{y-X\hat{\beta}})
\end{align*}
$$</p><h3 id=034--standard-errors>0.3.4 Standard Errors</h3><p>The variance-covariance matrix of $\hat{\mathbf{\beta}}$ is:
$$
Var(\hat{\mathbf{\beta}})=\sigma^{2}(\mathbf{X^{T}X})^{-1}
$$
Since $\sigma^{2}$ is unknown, we replace it by $s^{2}$ to obtain its (computable) estimate:
$$
\hat{Var}(\hat{\mathbf{\beta}})=s^{2}(\mathbf{X^{T}X})^{-1}
$$
The standard error of $\beta_{i},i=1,2,\cdots,p$ is the square root of the $i+1_{th}$ diagonal element of $\hat{Var}(\hat{\mathbf{\beta}})$.</p><h3 id=035--confidence-intervals>0.3.5 Confidence Intervals</h3><p>$100(1 − α)%$ confidence intervals for $\beta_{i},i=1,2,\cdots,p$:
$$
\hat{\mathbf{\beta}}\ \pm\ t_{n-p-1,1-\frac{\alpha}{2}}\times s.e.(\hat{\beta_{i}})
$$</p><h3 id=036--hypotheses-testing>0.3.6 Hypotheses Testing</h3><p>Question: Is $x_{i}$ important for explaining / predicting $y$?</p><p>Form a hypothesis $H_{0}:\ \beta_{i}=0$ vs. $H_{1}:\ \beta_{i}\neq0$.</p><p>T-test:
$$
t-ratio=\frac{\hat{\mathbf{\beta}}<em>{i}-0}{s.e.(\hat{\mathbf{\beta</em>{i}}})}\mathop{\sim}\limits^{H_{0}}t(n-p-1)
$$
Reject $H_{0}$ if p-value of the test is small ($e.g.&lt; 0.05$).</p><h3 id=037--f-test>0.3.7 F Test</h3><p>Question:
$$
H_{0}: \beta_{1}=\beta_{2}=\cdots=\beta_{p}=0
$$
$$vs.$$
$$
H_{1}:\text{at least one }\beta_{i}\text{ is non-zero}
$$
$$
F=\frac{(\sum\limits_{i=1}^{n}(y_{i}-\overline{y})^{2} - \sum\limits_{i=1}^{n}(y_{i}-\hat{y_{i}})^{2})/p}{(\sum\limits_{i=1}^{n}(y_{i}-\hat{y_{i}})^{2})/(n-p-1)}\mathop{\sim}\limits^{H_{0}}F(p,n-p-1)
$$
This is called the <strong>analysis of variance (ANOVA)</strong>.</p><h3 id=038--variability-partition>0.3.8 Variability Partition</h3><p>$$
Total\ SS=Error\ SS+Regression\ SS
$$
$$
\sum\limits_{i=1}^{n}(y_{i}-\overline{y})^{2}=\sum\limits_{i=1}^{n}(y_{i}-\hat{y_{i}})^{2}+\sum\limits_{i=1}^{n}(\hat{y}_{i}-\overline{y})^{2}
$$
$$
R^{2}=\frac{Regression\ SS}{Total\ SS}
$$
$0\leq R^{2}\leq1$, the larger the better.</p><p>To consider the influence of model complexity, add the degree of freedom into consideration &ndash; <strong>adjusted $R^{2}$</strong>:
$$
\overline{R}^{2}=1-(1-R^{2})\frac{n-1}{n-k}
$$</p><p>Now we have:
$$
F=\frac{(\text{Total SS }-\text{ Error SS})/p}{(\text{Error SS})/(n-p-1)}=\frac{R^{2}/p}{(1-R^{2})/(n-p-1)}
$$
Degree of freedom:</p><table><thead><tr><th style=text-align:center>SS</th><th style=text-align:center>Degree</th></tr></thead><tbody><tr><td style=text-align:center>ESS</td><td style=text-align:center>p</td></tr><tr><td style=text-align:center>RSS</td><td style=text-align:center>n-p-1</td></tr><tr><td style=text-align:center>TSS</td><td style=text-align:center>n-1</td></tr></tbody></table><h3 id=039--predicting>0.3.9 Predicting</h3><p>Given $\boldsymbol{x} = \boldsymbol{x}^{*} \mathop{=}\limits^{def} (x_{1}^{*}, \cdots, x_{p}^{*})^\top$, what value would $y$ take?</p><p>Point prediction:
$$
\hat{y}^{*} = \hat{\beta}_0 + \hat{\beta}_1\times x^{*}_1 + \cdots + \hat{\beta}_p\times x^{*}_p
$$</p><p>$100(1 − \alpha)%$ prediction interval:
$$
\hat{y}^{*}\pm t_{n-2,1- \frac{\alpha}{2}}\times s.e.(pred)
$$
$$
s.e.(pred)=s\sqrt{1+(\mathbf{x}^{*})^{T}(\mathbf{X^{T}X})^{-1}\mathbf{x}^{*}}
$$</p><h2 id=04-collinearity>0.4. Collinearity</h2><h3 id=041--bad-influences>0.4.1 Bad influences</h3><ul><li>Larger values of OLS estamators&rsquo; variance and standard error.</li><li>Wider confidence interval.</li><li>Not significant t-value.</li><li>Higher $R^{2}$ but not all t values are significant.</li><li>Not robust, sensitive to the small change of data.</li></ul><h3 id=042--diagnoses>0.4.2 Diagnoses</h3><ul><li>Higher $R^{2}$ but the number of significant t values is small.</li><li>High correlation between variables.</li><li>Partial correlation coefficient.</li><li>Subsidiary or auxiliary regression, and test $R^{2}_{i}$.</li><li>Variance inflation factor VIF: $VIF=\frac{1}{1-R^{2}_{i}}$.</li></ul><h3 id=043--solutions>0.4.3 Solutions</h3><ul><li>Delete some variables.</li><li>More new data.</li><li>Reset model.</li><li>Variable transformation.</li><li>Factor analysis / principal component analysis / ridge regression / LASSO</li></ul><h2 id=05-heteroscedasticity--unequal-variance>0.5. Heteroscedasticity / unequal variance</h2><p>More frequent in corss-sectional data (due to the existence of scale effect).</p><h3 id=051--bad-influences>0.5.1 Bad influences</h3><ul><li>OLS estimators are still linear.</li><li>OLS estimators are still unbiased.</li><li>OLS estimators&rsquo; variance are not the smallest, so as to say they are no longer effective.</li><li>The variance of OLS estimators are biased, which is the result of a biased estimation of $\hat{\sigma}^{2}$.</li><li>The hypothesis test based on t-test or F-test is no longer reliabel.</li></ul><h3 id=052--diagnoses>0.5.2 Diagnoses</h3><ul><li>Graph of residuals.</li><li>Pake test<ul><li>OLS regression to get residuals.</li><li>Compute the square of residuals, and compute their ln.</li><li>Regression : $\ln e_{i}^{2}=B_{1}+B_{2}\ln X_{i}+v_{i}$ for every variable or for $\hat{Y}_{i}$.</li><li>Test zero hypothesis: $B_{2}=0$, which is equal to no heteroscedasticity.</li><li>If we can&rsquo;t reject zero hypothesis, $B_{1}$ can be seen a give value of equal variance.</li></ul></li><li>Glejser test<ul><li>it is similar to Pake test, but has three regressions.</li><li>$|e_{i}|=B_{1}+B_{2}X_{i}+v_{i}$.</li><li>$|e_{i}|=B_{1}+B_{2}\sqrt{X_{i}}+v_{i}$.</li><li>$|e_{i}|=B_{1}+B_{2}(\frac{1}{X_{i}})+v_{i}$</li><li>If all $B_{2}=0$, accept the hypothesis that no unequal variance.</li></ul></li><li>White&rsquo;s general test of heteroscedasticity<ul><li>For $Y_{i}=B_{1}+B_{2}X_{2i}+B_{3}X_{3i}+u_{i}$</li><li>OLS regression to get $e_{i}$</li><li>Regression $e_{i}^{2}=A_{1}+A_{2}X_{2i}+A_{3}X_{3i}+A_{4}X_{2i}^{2}+A_{5}X_{3i}^{2}+A_{6}X_{2i}X_{3i}+v_{i}$, so as to say regress $e_{i}^{2}$ for all original variables, higher powers of variables, cross terms of variables</li><li>Compute this regression&rsquo;s $R^{2}$. Then $n\cdot R^{2}\sim\chi^{2}_{k-1}$, $k=6$ in this case.</li><li>Zero-hytothesis: no unequal variance.</li></ul></li></ul><h3 id=053--solutions>0.5.3 Solutions</h3><ul><li>$\sigma^{2}_{i}$ is known<ul><li><strong>Weighted Least Squares (WLS)</strong>, divide the original regression function by $\sigma_{i}$.</li></ul></li><li>$\sigma_{i}^{2}$ is unknown<ul><li>While $E(u_{i}^{2})=\sigma^{2}X_{i}$ or $E(u_{i}^{2})=\sigma^{2}X_{i}^{2}$, divide the original regression function by $\sqrt{X_{i}}$ or $X_{i}$. (<strong>Still WLS</strong>, $u_{i}$ is the error of original regression).</li><li>These methods are also called <strong>variance stabilizing transformations</strong>.</li><li>Standard error and t-value after White heteroscedasticity adjusted.</li></ul></li></ul><h2 id=06-autocorrelation>0.6. Autocorrelation</h2><h3 id=061--bad-influences>0.6.1 Bad Influences</h3><ul><li>OLS estimators are still linear and unbiased.</li><li>OLS estimators are no longer effective.</li><li>The variance of OLS estimators are biased.</li><li>The hypothesis test based on t-test or F-test is no longer reliabel.</li><li>The variance of erros $\hat{\sigma}^{2}$ is biased (usually downside biased).</li><li>$R^{2}$ is not reliabel.</li><li>The prediction variance and std are not effective.</li></ul><h3 id=062--diagnoses>0.6.2 Diagnoses</h3><ul><li>Graph.</li><li>Durbin-Watson d test: $$d=\frac{\sum\limits_{t=2}^{n}(e_{t}-e_{t-1})^{2}}{\sum\limits_{t=1}^{n}e_{t}^{2}}$$<ul><li>Requests:<ul><li>Regression model includes intercept.</li><li>$X$ are nonstochastic.</li><li>Error term $u_{i}$ follows: $u_{t}=\rho u_{t-1}+v_{t}\quad -1\leq\rho\leq1$</li><li>$\rho$ is called <strong>coefficient of autocorrelation</strong>. This equation is called <strong>Markov first-order autoregressive scheme</strong>, deneted as $AR(1)$.</li><li>Variables doesn&rsquo;t include the lag term of $Y$ (not autoregressive models).</li></ul></li><li>Large sample &ndash; $d\approx2(1-\hat{\rho}),\hat{\rho}=\frac{\sum\limits_{t=2}^{n}e_{t}e_{t-1}}{\sum\limits_{t=1}^{n}e_{t}^{2}}$. So $0\leq d\leq4$.</li><li>$\hat{\rho}\rightarrow -1\text{ (negative correlation)},d\rightarrow 4$.</li><li>$\hat{\rho}\rightarrow 0\text{ (no correlation)},d\rightarrow2$.</li><li>$\hat{\rho}\rightarrow 1\text{ (positive correlation)},d\rightarrow 0$.</li><li>There are two critical value $d_{L},d_{U}$.</li></ul></li></ul><img src=/posts/linear_models/lr/DB-d%20test.png alt="DB d test" width=600 height=400 class=center><div style=margin-top:3rem></div><h3 id=063--solutions>0.6.3 Solutions</h3><ul><li><strong>GLS</strong><ul><li>Suppose error term follows $AR(1)$: $u_{t}=\rho u_{t-1}+v_{t}$.</li><li>OLS regress $Y_{t}^{*}=B_{1}^{*}+B_{2}^{*}X_{t}^{*}+v_{t}$.</li><li>$Y_{t}^{*}=Y_{t}-\rho Y_{t-1}$, the others are similar.</li><li>This method is called <strong>Generalized Least Squares, GLS</strong>, this equation is called <strong>Generalized Difference Equation</strong>.</li><li>The first instance is lost in this difference equation, we can transform it using this fomular (<strong>Prais-Winsten transformation</strong>):</li><li>$Y_{1}^{*}=\sqrt{1-\rho^{2}}(Y_{1}),X_{1}^{*}=\sqrt{1-\rho^{2}}(X_{1})$.</li><li>The estimation of $\rho$.<ul><li>$\rho=1$: first-order difference method, suppose error term are positively correlated.</li><li>Estimate $\rho$ from Durbin-Watson d statistic. $d\approx2(1-\hat{\rho})\Rightarrow\hat{\rho}\approx1-\frac{d}{2}$.</li><li>Estimate $\rho$ from OLS residuals $e_{t}$: $e_{t}=\hat{\rho}e_{t-1}+v_{t}$.</li></ul></li></ul></li><li><strong>Large sample method: Newey-West method</strong><ul><li>Also HAC std. It doesn&rsquo;t advise the values of OLS estimators, but just advise their stds.</li></ul></li></ul></div><div class="row ps-3 pe-3"><div class="col-md-6 share-buttons"><strong>Share on:</strong>
<a class="btn icon-button bg-facebook" href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fonline727.github.io%2fposts%2flinear_models%2flr%2f" target=_blank><i class="fab fa-facebook"></i>
</a><a class="btn icon-button bg-twitter" href="https://twitter.com/share?url=https%3a%2f%2fonline727.github.io%2fposts%2flinear_models%2flr%2f&text=Linear%20Regression&via=Haohan%27s%20Blog" target=_blank><i class="fab fa-twitter"></i>
</a><a class="btn icon-button bg-reddit" href="https://reddit.com/submit?url=https%3a%2f%2fonline727.github.io%2fposts%2flinear_models%2flr%2f&title=Linear%20Regression" target=_blank><i class="fab fa-reddit"></i>
</a><a class="btn icon-button bg-linkedin" href="https://www.linkedin.com/shareArticle?url=https%3a%2f%2fonline727.github.io%2fposts%2flinear_models%2flr%2f&title=Linear%20Regression" target=_blank><i class="fab fa-linkedin"></i>
</a><a class="btn icon-button bg-whatsapp" href="https://api.whatsapp.com/send?text=Linear%20Regression https%3a%2f%2fonline727.github.io%2fposts%2flinear_models%2flr%2f" target=_blank><i class="fab fa-whatsapp"></i>
</a><a class="btn icon-button" href="mailto:?subject=Linear%20Regression&body=https%3a%2f%2fonline727.github.io%2fposts%2flinear_models%2flr%2f" target=_blank><i class="fas fa-envelope-open-text"></i></a></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/online727/online727.github.io/edit/main/content/posts/linear_models/lr/index.md title="Improve this page" target=_blank rel=noopener><i class="fas fa-code-branch"></i>
Improve this page</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/posts/quant/multi-factors/perf-attri/ title="Performance Attribution" class="btn filled-button"><div><i class="fas fa-chevron-circle-left"></i> Prev</div><div class=next-prev-text>Performance Attribution</div></a></div><div class="col-md-6 next-article"><a href=/posts/matrix_cookbook/basic/notations/ title=Notations class="btn filled-button"><div>Next <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>Notations</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn type=button data-bs-toggle=tooltip data-bs-placement=left title="Scroll to top"><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center ps-3">Table of Contents</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#01-general-expression>0.1 General Expression</a></li><li><a href=#02-ols-assumptions>0.2 OLS Assumptions</a></li><li><a href=#03-ols-estimators>0.3 OLS Estimators</a><ul><li><a href=#031--estimators-of-hatbeta>0.3.1 Estimators of $\hat{\beta}$</a></li><li><a href=#032--properties-of-ols-estimators>0.3.2 Properties of OLS estimators:</a></li><li><a href=#033--estimators-of-sigma2>0.3.3 Estimators of $\sigma^{2}$</a></li><li><a href=#034--standard-errors>0.3.4 Standard Errors</a></li><li><a href=#035--confidence-intervals>0.3.5 Confidence Intervals</a></li><li><a href=#036--hypotheses-testing>0.3.6 Hypotheses Testing</a></li><li><a href=#037--f-test>0.3.7 F Test</a></li><li><a href=#038--variability-partition>0.3.8 Variability Partition</a></li><li><a href=#039--predicting>0.3.9 Predicting</a></li></ul></li><li><a href=#04-collinearity>0.4. Collinearity</a><ul><li><a href=#041--bad-influences>0.4.1 Bad influences</a></li><li><a href=#042--diagnoses>0.4.2 Diagnoses</a></li><li><a href=#043--solutions>0.4.3 Solutions</a></li></ul></li><li><a href=#05-heteroscedasticity--unequal-variance>0.5. Heteroscedasticity / unequal variance</a><ul><li><a href=#051--bad-influences>0.5.1 Bad influences</a></li><li><a href=#052--diagnoses>0.5.2 Diagnoses</a></li><li><a href=#053--solutions>0.5.3 Solutions</a></li></ul></li><li><a href=#06-autocorrelation>0.6. Autocorrelation</a><ul><li><a href=#061--bad-influences>0.6.1 Bad Influences</a></li><li><a href=#062--diagnoses>0.6.2 Diagnoses</a></li><li><a href=#063--solutions>0.6.3 Solutions</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-start"><div class="col-md-4 col-sm-12"><h5>Navigation</h5><ul><li class=nav-item><a class=smooth-scroll href=https://online727.github.io/#about>About</a></li><li class=nav-item><a class=smooth-scroll href=https://online727.github.io/#education>Education</a></li><li class=nav-item><a class=smooth-scroll href=https://online727.github.io/#experiences>Experiences</a></li><li class=nav-item><a class=smooth-scroll href=https://online727.github.io/#projects>Projects</a></li><li class=nav-item><a class=smooth-scroll href=https://online727.github.io/#skills>Skills</a></li><li class=nav-item><a class=smooth-scroll href=https://online727.github.io/#featured-posts>Featured Posts</a></li><li class=nav-item><a class=smooth-scroll href=https://online727.github.io/#recent-posts>Recent Posts</a></li><li class=nav-item><a class=smooth-scroll href=https://online727.github.io/#accomplishments>Accomplishments</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>Contact me:</h5><ul><li><a href=mailto:zhhohoh27@gamil.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>zhhohoh27@gamil.com</span></a></li><li><a href=https://github.com/online727 target=_blank rel=noopener><span><i class="fab fa-github"></i></span> <span>online727</span></a></li><li><a href=https://www.linkedin.com/in/haohan-zhao-9a6b24296 target=_blank rel=noopener><span><i class="fab fa-linkedin"></i></span> <span>Haohan Zhao</span></a></li><li><span><i class="fas fa-phone-alt"></i></span> <span>+86 19551998168</span></li></ul></div></div></div><hr><div class=container><p id=disclaimer><strong>Liability Notice:</strong> This site is built on hugo and github pages and uses the hugo-toha theme. The site is used for personal blogging, all content is owned by me and does not constitute any relevant advice, if you have any questions, please contact me!</p></div><hr><div class=container><div class="row text-start"><div class=col-md-4><a id=theme href=https://github.com/hugo-toha/toha target=_blank rel=noopener><img src=/images/theme-logo_hu16779671404603505019.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 Copyright.</div><div class="col-md-4 text-end"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/application.aa1b29568827d862abc07cd2a25fa9f3f1dad96fd8fe2a012bccae2de39367fc.js integrity="sha256-qhspVogn2GKrwHzSol+p8/Ha2W/Y/ioBK8yuLeOTZ/w=" defer></script></body></html>