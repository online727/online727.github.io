[{"categories":null,"contents":" Strings str := \u0026#34;Hello\u0026#34; Multiline string\nstr := `Multiline string` Numbers Typical types\nnum := 3 // int num := 3. // float64 num := 3 + 4i // complex128 num := byte(\u0026#39;a\u0026#39;) // byte (alias for uint8) Other Types\nvar u uint = 7 // uint (unsigned) var p float32 = 22.7 // 32-bit float Arrays // var numbers [5]int numbers := [...]int{0, 0, 0, 0, 0} Pointers func main () { b := *getPointer() fmt.Println(\u0026#34;Value is\u0026#34;, b) func getPointer () (myPointer *int) { a := 234 return \u0026amp;a a := new(int) *a = 234 Pointers point to a memory location of a variable. Go is fully garbage-collected.\nType Conversion i := 2 f := float64(i) u := uint(i) Slice slice := []int{2, 3, 4} slice := []byte(\u0026#34;Hello\u0026#34;) ","date":"January 1, 0001","hero":"/images/default-hero.jpg","permalink":"https://online727.github.io/cn/notes/example/testnotes/simpletest/","summary":"Strings str := \u0026#34;Hello\u0026#34; Multiline string\nstr := `Multiline string` Numbers Typical types\nnum := 3 // int num := 3. // float64 num := 3 + 4i // complex128 num := byte(\u0026#39;a\u0026#39;) // byte (alias for uint8) Other Types\nvar u uint = 7 // uint (unsigned) var p float32 = 22.7 // 32-bit float Arrays // var numbers [5]int numbers := [...]int{0, 0, 0, 0, 0} Pointers func main () { b := *getPointer() fmt.","tags":null,"title":"简单测试"},{"categories":null,"contents":"接下来一段时间，我想自学深度学习，使用的教材为 动手学深度学习 (Pytorch 版)，该书有线上网址，且提供配套代码和相关 Python 包，详情可参见 动手学深度学习。\n第一章内容，介绍了深度学习的相关背景和应用场景，以及深度学习领域常见的术语和名词，有一定机器学习经验的人或许已比较熟悉，故不再赘述，我们直接从第二章开始。\n1. Tensor 操作和数据预处理 深度学习中的数据以张量 (tensor) 形式存储，支持 GPU 计算和 autograd 自动微分。\n张量的创建、变形、运算 (按元素 / 矩阵)、广播机制、索引、切片等均与 numpy.ndarray 类似。\n节省内存： Y = X + Y 不是原地操作，即：id(Y = X + Y) != id(Y)，会分配新的内存。 使用 Y[:] = X + Y 或 Y += X 进行原地操作以避免不必要的内存分配。 Tensor 可以与其他 Python 对象互相转换，如 tensor.numpy()。大小为 1 的张量可以转化为 Python 标量，使用 tensor.item() 或 float(tensor) 等。\n数据需要经过预处理，如填充 nan，标准化等，可以借用其他 Python 包处理后再转化为 tensor。\n2. 线性代数 标量，以小写字母 $x,y,z$ 等表示。 向量，以粗体小写字母 $\\bold{x,y,z}$ 表示，向量的维度 (形状) 代表元素个数 (向量长度)，可以使用 len(x), x.shape 获取。以列向量为默认的向量方向，例如： $$ \\begin{equation*} x = \\begin{bmatrix*} x_{1} \\cr x_{2} \\cr \\vdots \\cr x_{n} \\end{bmatrix*} \\end{equation*} $$ 矩阵，以粗体大写字母 $\\bold{X,Y,Z}$ 表示，是具有两个轴的张量。 张量 (此处指代数对象)，矩阵的拓展，一种具有更多轴的数据结构，使用特殊字体的大写字母 $X, Y, Z$ 表示。 张量的计算，与 numpy.ndarray 相同，普通的加减乘除、求和、平均、向量点积、矩阵 hadamard 积、矩阵-向量积、矩阵乘法、范数等。\n3. 微积分 3.1 导数和微分 设 $y=f(x)$，其导数被定义为： $$ f^{\u0026rsquo;}(x) = \\lim_{h\\rightarrow 0}\\frac{f(x+h) - f(x)}{h} $$\n以下符号等价： $$ f^{\u0026rsquo;}(x)=y^{\u0026rsquo;}=\\frac{dy}{dx}=\\frac{df}{dx}=\\frac{d}{dx}f(x)=Df(x)=D_{x}f(x) $$\n自行回顾求导法则。\n3.2 偏导数 将微分的思想拓展至多元函数上，设 $y=f(x_{1},\\cdots,x_{n})$ 是一个 $n$ 元函数，其关于第 $i$ 个变量 $x_{i}$ 的偏导数为： $$ \\frac{\\partial y}{\\partial x_{i}} = \\lim_{h\\rightarrow 0}\\frac{f(x_{1},\\cdots,x_{i-1},x_{i}+h,x_{i+1},\\cdots,x_{n}) - f(x_{1},\\cdots,x_{i},\\cdot,x_{n})}{h} $$\n以下表达等价： $$ \\frac{\\partial y}{\\partial x_{i}} = \\frac{\\partial f}{\\partial x_{i}} = f_{x_{i}} = f_{i} = D_{i}f = D_{x_{i}}f $$\n3.3 梯度 函数的梯度（gradient）向量，即其对所有变量的偏导数。设函数 $f: \\mathbb{R}^n \\to \\mathbb{R}$ 的输入是一个 $n$ 维向量 $\\mathbf{x} = [x_1, x_2, \\ldots, x_n]$，并且输出是一个标量。函数 $f(\\mathbf{x})$ 相对于 $\\mathbf{x}$ 的梯度是一个包含 $n$ 个偏导数的向量： $$ \\nabla_{\\mathbf{x}} f(\\mathbf{x}) = \\begin{bmatrix} \\frac{\\partial f(\\mathbf{x})}{\\partial x_1} \u0026amp; \\frac{\\partial f(\\mathbf{x})}{\\partial x_2} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f(\\mathbf{x})}{\\partial x_n} \\end{bmatrix}^\\top, $$\n假设 $\\mathbf{x}$ 为 $n$ 维向量，在微分多元函数时经常使用以下规则：\n对于所有 $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$，都有 $\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} = \\mathbf{A}^\\top$ 对于所有 $\\mathbf{A} \\in \\mathbb{R}^{n \\times m}$，都有 $\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} = \\mathbf{A}$ 对于所有 $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$，都有 $\\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{A} \\mathbf{x} = (\\mathbf{A} + \\mathbf{A}^\\top) \\mathbf{x}$ $\\nabla_{\\mathbf{x}} |\\mathbf{x}|^2 = \\nabla_{\\mathbf{x}} \\mathbf{x}^\\top \\mathbf{x} = 2 \\mathbf{x}$ 同样，对于任意矩阵 $\\mathbf{X}$，都有 $\\nabla_{\\mathbf{X}} |\\mathbf{X}|_F^2 = 2 \\mathbf{X}$。\n3.4 链式法则 考虑单变量函数 $y = f(u)$ 和 $u = g(x)$， 假设都是可微的，根据链式法则： $$ \\frac{dy}{dx} = \\frac{dy}{du} \\cdot \\frac{du}{dx} $$\n当函数具有任意数量的变量时，假设可微分函数 $y$ 有变量 $u_1, u_2, \\cdots, u_m$，其中每个可微分函数 $u_i$ 都有变量 $x_1, x_2, \\cdots, x_n$。注意，$y$ 是 $x_1, x_2, \\cdots, x_n$ 的函数。对于任意 $i = 1, 2, \\cdots, n$，链式法则给出：\n$$ \\frac{\\partial y}{\\partial x_i} = \\frac{\\partial y}{\\partial u_1} \\frac{\\partial u_1}{\\partial x_i} + \\frac{\\partial y}{\\partial u_2} \\frac{\\partial u_2}{\\partial x_i} + \\cdots + \\frac{\\partial y}{\\partial u_m} \\frac{\\partial u_m}{\\partial x_i}. $$\n4. 自动微分 Pytorch 使用自动微分 (automatic differentiation) 来加快求导。 实际中，根据设计好的模型，系统会构建一个计算图 (computational graph)，以跟踪计算是哪些数据通过哪些操作组合起来产生输出，并使用自动微分进行反向传播梯度。 这里，反向传播 (backpropagate) 意味着跟踪整个计算图，填充关于每个参数的偏导数。\n当 $y$ 是标量时，可以通过链式法则反向求导输入参数的梯度，该梯度是一个与输入向量 $\\bold{x}$ 形状相同的向量。\n当 $y$ 不是标量时，向量 $\\bold{y}$ 关于 $\\bold{x}$ 的导数是一个矩阵，更高阶情况下是一个高阶张量。但当调用向量的反向传播计算时，通常会试图计算一批训练样本中每个组成部分的损失函数的导数。 这里、的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。如：\nx = torch.arange(4.0) y = x * x y.sum().backward() # y.backward(torch.ones(len(x))) 如果想将某些计算移到记录的计算图之外，例如，假设 $y$ 是作为 $x$ 的函数计算的，而 $z$ 则是作为 $y$ 和 $x$ 的函数计算的。比如，我们想计算 $z$ 关于 $x$ 的梯度，但由于某种原因，希望将 $y$ 视为一个常数，并且只考虑到 $x$ 在 $y$ 被计算后发挥的作用。\n此时，可以使用 detach() 分离 $y$ 来返回一个新变量 $u$，该变量与 $y$ 具有相同的值，但丢弃计算图中如何计算 $y$ 的任何信息。换句话说，梯度不会向后流经 $u$ 到 $x$。因此，下面的反向传播函数计算 $z=u*x$ 关于 $x$ 的偏导数，$u$ 为常数。\nx.grad.zero_() y = x * x u = y.detach() z = u * x z.sum().backward() 5. 概率 单个随机变量：概率分布 多个随机变量 联合概率 (联合分布) 条件概率 (条件分布) 贝叶斯定理 边际化 (边际概率、边际分布 \u0026ndash; 全概率公式) $$P(B)=\\sum_{A}P(A,B)$$ 独立性 期望和方差 ","date":"July 27, 2024","hero":"/images/default-hero.jpg","permalink":"https://online727.github.io/cn/posts/dltorch/ch2/","summary":"接下来一段时间，我想自学深度学习，使用的教材为 动手学深度学习 (Pytorch 版)，该书有线上网址，且提供配套代码和相关 Python 包，详情可参见 动手学深度学习。\n第一章内容，介绍了深度学习的相关背景和应用场景，以及深度学习领域常见的术语和名词，有一定机器学习经验的人或许已比较熟悉，故不再赘述，我们直接从第二章开始。\n1. Tensor 操作和数据预处理 深度学习中的数据以张量 (tensor) 形式存储，支持 GPU 计算和 autograd 自动微分。\n张量的创建、变形、运算 (按元素 / 矩阵)、广播机制、索引、切片等均与 numpy.ndarray 类似。\n节省内存： Y = X + Y 不是原地操作，即：id(Y = X + Y) != id(Y)，会分配新的内存。 使用 Y[:] = X + Y 或 Y += X 进行原地操作以避免不必要的内存分配。 Tensor 可以与其他 Python 对象互相转换，如 tensor.numpy()。大小为 1 的张量可以转化为 Python 标量，使用 tensor.item() 或 float(tensor) 等。\n数据需要经过预处理，如填充 nan，标准化等，可以借用其他 Python 包处理后再转化为 tensor。\n2. 线性代数 标量，以小写字母 $x,y,z$ 等表示。 向量，以粗体小写字母 $\\bold{x,y,z}$ 表示，向量的维度 (形状) 代表元素个数 (向量长度)，可以使用 len(x), x.","tags":["深度学习","Pytorch"],"title":"第二章 预备知识"},{"categories":null,"contents":"1. 摘要 投资组合的业绩归因可以分为 收益归因 和 风险归因 两个部分，而归因又可以基于净值或持仓进行。本文主要基于 持仓数据 对组合的业绩归因进行探讨。\n在组合收益归因方面，主要有以下部分：\n基于 Brinson 模型 经典版 BHB (Brinson, Hood and Beebower) 模型：将组合超额收益分解为配置收益、选股收益和交互收益 3 部分。 改进版 BF (Brinson and Fachler) 模型：引入行业超额收益，将组合超额收益分解为配置效应和选股效应两个部分。 基于多因子模型 基于行业的多因子收益归因：与自下而上的 Brinson 模型完全一致。 基于行业和风格的多因子收益归因：同时对行业和风格上的配置进行分析。 在组合风险归因方面，主要基于多因子模型：\n单一波动分解法：单独考虑每个因子，计算简单，但忽略因子之间的协同影响，且不具可加性。 边际风险分解法：将组合风险分解为因子暴露度与因子边际风险贡献的乘积，然而偏导数的概念相对模糊，指导意义不强。 三要素分解法：将风险分解为因子暴露 ($x$)、因子波动 ($\\sigma$) 和因子-组合相关系数 ($\\rho$)，对风险的分解更为透彻，更有利于投资经理对风险进行控制。 2. 基于 Brinson 模型的组合收益归因 2.1 经典 BHB 模型 BHB 模型将投资组合的超额收益率分解为 配置收益、选股收益和交互收益 三个部分，其基本框架如下图所示，其中红色渲染部分表示投资组合的超额收益。\n从行业配置的角度而言，假设 $w_{i}^{P}, w_{i}^{B}$ 分别表示投资组合和基准组合中行业 $i$ 的权重，$r_{i}^{P}, r_{i}^{B}$ 分别表示投资组合和基准组合中行业 $i$ 的收益率，那么投资组合的收益率 $R^{P}$ 和基准组合的收益率 $R^{B}$ 就可以表示为： $$ \\begin{align*} R^{P}\u0026amp;=\\sum_{i=1}^{I}w_{i}^{P}r_{i}^{P}, where\\sum_{i=1}^{I}w_{i}^{P}=1 \\cr R^{B}\u0026amp;=\\sum_{i=1}^{I}w_{i}^{B}r_{i}^{B}, where\\sum_{i=1}^{I}w_{i}^{B}=1 \\end{align*} $$\n其中，$I$ 表示行业个数。此时，投资组合超额收益 $R^{A}$ 即可表示为： $$ R^{A}=R^{P}-R^{B}=\\sum_{i=1}^{I}w_{i}^{P}r_{i}^{P} - \\sum_{i=1}^{I}w_{i}^{B}r_{i}^{B} $$\nBHB 模型将组合超额收益拆解为 配置收益 (Allocation Return, AR)、选股收益 (Selection Return, SR) 和 交互收益 (Interaction Return, IR) 三部分，即： $$ \\begin{align*} R^{A} \u0026amp;= AR+SR+IR \\cr AR \u0026amp;= \\sum_{i=1}^{I}(w_{i}^{P} - w_{i}^{B})r_{i}^{B} \\cr SR \u0026amp;= \\sum_{i=1}^{I}w_{i}^{B}(r_{i}^{P} - r_{i}^{B}) \\cr IR \u0026amp;= R^{A} - AR - SR = \\sum_{i=1}^{I}(w_{i}^{P} - w_{i}^{B})(r_{i}^{P} - r_{i}^{B}) \\end{align*} $$\n配置收益 (AR) 等于投资组合在每个行业上的超额权重与基准行业收益率的乘积 (AR = 超额权重 × 基准行业收益率)，表示在行业内部不进行任何选股操作，持有与基准组合完全相同的行业，并通过超配收益为正、低配收益为负的行业所能够获取的超额收益。 选择收益 (SR) 等于基准权重与投资组合在行业上超额收益率的乘积 (SR = 基准权重 × 行业超额收益) ，表示在组合中保持每个行业权重与基准指数行业权重完全一致，通过行业内部的选股操作所能够获取的超额收益。 交互收益 (IR) 等于超额权重与超额收益的乘积 (IR = 超额权重 × 超额收益)，表示由配置和选股共同产生的超额收益。 本文的所有示例都是按照行业进行划分的，但 Brinson 模型的应用远不止于此。对于大类资产配置的投资者而言，他可以将收益拆解到股票、债券、现金、基金和衍生品等不同类别的大类资产配置和选择带来的收益，对于债券投资者而言，他可以将超额收益拆解到企业债、信用债、利率债、国债等不同债券类别的配置和选择带来的收益。\nBHB 模型存在的不足：\n配置效应 $AR = \\sum_{i=1}^{I}(w_{i}^{P} - w_{i}^{B})r_{i}^{B}$ 中，BHB 模型 认为超配行业绝对收益为正的行业即可获得配置效应，但若某些行业只是具有正收益，但却未能胜过基准组合，即：$0 \u0026lt; r_{i}^{B} \u0026lt; R^{B}$，那么超配此类行业显然不是完全成功的。 选择效应 $SR = \\sum_{i=1}^{I}w_{i}^{B}(r_{i}^{P} - r_{i}^{B})$ 中，当投资组合中某行业相对基准行业存在超额收益，但投资组合对该行业的权重配置低于基准权重配置，即：$w_{i}^{P} \u0026lt; w_{i}^{B}$，若仍按基准权重来计算选择效应，结果会存在一定高估。 交互效应 $IR = \\sum_{i=1}^{I}(w_{i}^{P} - w_{i}^{B})(r_{i}^{P} - r_{i}^{B})$ 的概念相对模糊，配置效应是对绝对收益为正的行业的超配、对绝对收益为负的行业的低配带来的收益，选股效应是对超额收益为正的个股的超配、对超额收益为负的个股低配带来的收益，但交互项收益部分很难从操作层面去解释，为组合的管理带来了难题。 2.2 改进版 BF 模型 BF 模型是 BHB 模型的改进版，增加了基准收益 $R^{B}$ 对配置收益的影响，基本框架如下图所示，其中仍以红色渲染部分表示投资组合的超额收益。\nBF 模型在配置效应部分的计算引入了基准收益 $R^{B}$，新的配置效应可以表示为： $$ AR_{BF}=\\sum_{i=1}^{I}(w_{i}^{P} - w_{i}^{B})(r_{i}^{B} - R^{B}) $$\n由于资产组合权重 $w_{i}^{P}$ 和基准组合权重 $w_{i}^{B}$ 加总均为 1，而 $R^{B}$ 是一个常数，因此： $$ \\begin{gather*} \\sum_{i=1}^{I}(w_{i}^{P} - w_{i}^{B})R^{B} = 0 \\cr AR_{BHB} = \\sum_{i=1}^{I}(w_{i}^{P} - w_{i}^{B})r_{i}^{B} = \\sum_{i=1}^{I}(w_{i}^{P} - w_{i}^{B})(r_{i}^{B} - R^{B}) = AR_{BF} \\end{gather*} $$\n也就是说，相较于 BHB 模型而言，BF 模型中对于基准收益的引入并不会改变其配置效应的大小，二者是完全等同的，但在直观解释上 BF 模型却与投资者的实际操作更为贴合，它认为：只有超配那些相较基准指数具有正向超额收益的行业、低配那些相较基准指数具有负向超额收益的行业，才能算是成功的行业配置策略。\nBF 模型还将 BHB 模型中的选择效应和交互效应进行了合并，形成新的选股效应： $$ \\begin{align*} SR_{BF} \u0026amp;= SR_{BHB} + IR_{BHB} \\cr \u0026amp;= \\sum_{i=1}^{I}w_{i}^{B}(r_{i}^{P} - r_{i}^{B}) + \\sum_{i=1}^{I}(w_{i}^{P} - w_{i}^{B})(r_{i}^{P} - r_{i}^{B}) \\cr \u0026amp;= \\sum_{i=1}^{I}w_{i}^{P}(r_{i}^{P} - r_{i}^{B}) \\end{align*} $$\n最后，BF 模型就将投资组合的超额收益 $R^{A}$ 分解到了对行业的配置效应 $AR$ 和行业内部的选股效应 $SR$ 两个部分： $$ \\begin{equation*} R^{A}{BF} = AR{BF} + SR_{BF} = \\sum\\limits_{i=1}^{I}(w_{i}^{P} - w_{i}^{B})(r_{i}^{B} - R^{B}) + \\sum\\limits_{i=1}^{I}w_{i}^{P}(r_{i}^{P} - r_{i}^{B}) \\end{equation*} $$\n下面，我们开始\nReference: 《“星火”多因子专题报告（四） —— 基于持仓的基金业绩归因：始于 Brinson，归于 Barra》，财通证券。\n","date":"July 23, 2024","hero":"/posts/quant/multi-factors/perf-attri/images/boat.jpg","permalink":"https://online727.github.io/cn/posts/quant/multi-factors/perf-attri/","summary":"1. 摘要 投资组合的业绩归因可以分为 收益归因 和 风险归因 两个部分，而归因又可以基于净值或持仓进行。本文主要基于 持仓数据 对组合的业绩归因进行探讨。\n在组合收益归因方面，主要有以下部分：\n基于 Brinson 模型 经典版 BHB (Brinson, Hood and Beebower) 模型：将组合超额收益分解为配置收益、选股收益和交互收益 3 部分。 改进版 BF (Brinson and Fachler) 模型：引入行业超额收益，将组合超额收益分解为配置效应和选股效应两个部分。 基于多因子模型 基于行业的多因子收益归因：与自下而上的 Brinson 模型完全一致。 基于行业和风格的多因子收益归因：同时对行业和风格上的配置进行分析。 在组合风险归因方面，主要基于多因子模型：\n单一波动分解法：单独考虑每个因子，计算简单，但忽略因子之间的协同影响，且不具可加性。 边际风险分解法：将组合风险分解为因子暴露度与因子边际风险贡献的乘积，然而偏导数的概念相对模糊，指导意义不强。 三要素分解法：将风险分解为因子暴露 ($x$)、因子波动 ($\\sigma$) 和因子-组合相关系数 ($\\rho$)，对风险的分解更为透彻，更有利于投资经理对风险进行控制。 2. 基于 Brinson 模型的组合收益归因 2.1 经典 BHB 模型 BHB 模型将投资组合的超额收益率分解为 配置收益、选股收益和交互收益 三个部分，其基本框架如下图所示，其中红色渲染部分表示投资组合的超额收益。\n从行业配置的角度而言，假设 $w_{i}^{P}, w_{i}^{B}$ 分别表示投资组合和基准组合中行业 $i$ 的权重，$r_{i}^{P}, r_{i}^{B}$ 分别表示投资组合和基准组合中行业 $i$ 的收益率，那么投资组合的收益率 $R^{P}$ 和基准组合的收益率 $R^{B}$ 就可以表示为： $$ \\begin{align*} R^{P}\u0026amp;=\\sum_{i=1}^{I}w_{i}^{P}r_{i}^{P}, where\\sum_{i=1}^{I}w_{i}^{P}=1 \\cr R^{B}\u0026amp;=\\sum_{i=1}^{I}w_{i}^{B}r_{i}^{B}, where\\sum_{i=1}^{I}w_{i}^{B}=1 \\end{align*} $$","tags":["绩效归因","多因子模型"],"title":"多因子绩效归因"}]