<!doctype html><html lang=cn><head><title>博文</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=/application.9b9342080d6c469bf44748bc72b47ec075a124fe0c6e6ff0f13d20c757e4d5a9.css integrity="sha256-m5NCCA1sRpv0R0i8crR+wHWhJP4Mbm/w8T0gx1fk1ak="><link rel=icon type=image/png href=/images/site/favicon_hu8414222332455362891.png><meta property="og:url" content="https://online727.github.io/cn/posts/"><meta property="og:site_name" content="赵浩翰 - 博客"><meta property="og:title" content="博文"><meta property="og:locale" content="cn"><meta property="og:type" content="website"><meta name=twitter:card content="summary"><meta name=twitter:title content="博文"><script>theme=localStorage.getItem("darkmode:color-scheme")||"system",theme=="system"&&(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?theme="dark":theme="light"),document.documentElement.setAttribute("data-theme",theme)</script></head><body class="type-posts kind-section" data-bs-spy=scroll data-bs-target=#TableOfContents data-bs-offset=80><div class="container-fluid bg-secondary wrapper"><nav class="navbar navbar-expand-xl top-navbar shadow" id=top-navbar><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button>
<i data-feather=sidebar></i>
</button>
<a class=navbar-brand href=/cn><img src=/images/site/main-logo_hu10708377409321002774.png id=logo alt=Logo>
赵浩翰 - 博客</a>
<button class="navbar-toggler navbar-light" id=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#top-nav-items aria-label=menu>
<i data-feather=menu></i></button><div class="collapse navbar-collapse dynamic-navbar" id=top-nav-items><ul class="nav navbar-nav ms-auto"><li class=nav-item><a class=nav-link href=/cn#home>主页</a></li><li class=nav-item><a class=nav-link href=/cn#about>关于</a></li><li class=nav-item><a class=nav-link href=/cn#skills>技能</a></li><li class=nav-item><a class=nav-link href=/cn#experiences>经历</a></li><li class=nav-item><a class=nav-link href=/cn#education>教育</a></li><div id=top-navbar-divider></div><li class=nav-item><a class=nav-link id=blog-link href=/cn/posts>博文</a></li><li class=nav-item><a class=nav-link id=note-link href=/cn/notes>笔记</a></li><li class=nav-item><a class=nav-link href=https://toha-guides.netlify.app/posts/>文档</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=languageSelector role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false><span class="flag-icon flag-icon-cn"></span>
简体中文</a><div class=dropdown-menu aria-labelledby=languageSelector><a class="dropdown-item nav-link languages-item" href=/posts><span class="flag-icon flag-icon-gb"></span>
English
</a><a class="dropdown-item nav-link languages-item" href=/cn/posts><span class="flag-icon flag-icon-cn"></span>
简体中文
</a><a class="dropdown-item nav-link languages-item" href=/bn/posts><span class="flag-icon flag-icon-bd"></span>
বাংলা</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg class=theme-icon src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme"></a><div id=themeMenu class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# data-scheme=light><img class=theme-icon src=/icons/sun-svgrepo-com.svg width=20 alt="Light Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=dark><img class=theme-icon src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=system><img class=theme-icon src=/icons/computer-svgrepo-com.svg width=20 alt="System Theme"></a></div></li></ul></div></div><img src=/images/site/main-logo_hu10708377409321002774.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu8414222332455362891.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/cn/search><input type=text name=keyword placeholder=搜索 data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/cn/posts data-filter=all>博文</a></li><div class=subtree><li><i data-feather=plus-circle></i><a class=list-link href=/cn/posts/quant/> 量化</a><ul><li><i data-feather=plus-circle></i><a class=list-link href=/cn/posts/quant/multi-factors/> 多因子模型</a><ul><li><a class=list-link href=/cn/posts/quant/multi-factors/perf-attri/ title=多因子绩效归因>多因子绩效归因</a></li></ul></li></ul></li><li><i data-feather=plus-circle></i><a class=list-link href=/cn/posts/dltorch/> 深度学习 (Pytorch 版)</a><ul><li><a class=list-link href=/cn/posts/dltorch/ch2/ title="第二章 预备知识">第二章 预备知识</a></li><li><a class=list-link href=/cn/posts/dltorch/ch3/ title="第三章 线性神经网络">第三章 线性神经网络</a></li><li><a class=list-link href=/cn/posts/dltorch/ch4/ title="第四章 多层感知机">第四章 多层感知机</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class="content container-fluid" id=content><div class="container-fluid post-card-holder" id=post-card-holder><div class=post-card><div class=card><div class=card-head><a href=/cn/posts/dltorch/ch4/ class=post-card-link><img class=card-img-top src=/images/default-hero.jpg alt="Hero Image"></a></div><div class=card-body><a href=/cn/posts/dltorch/ch4/ class=post-card-link><h5 class=card-title>第四章 多层感知机</h5><p class="card-text post-summary"><p><strong>多层感知机</strong> 是最简单的深度网络，由多层神经元组成，每一层与它的上一层相连，从中接收输入；同时每一层也与它的下一层相连，影响当前层的神经元。本章还涉及许多基本的概念介绍，包括过拟合、欠拟合、模型选择、数值稳定性、参数初始化以及权重衰减和暂退法等正则化技术。</p><h2 id=1-多层感知机>1. 多层感知机</h2><h3 id=11-简介>1.1 简介</h3><p>在第三章中涉及了线性回归和 softmax 回归，并在线性的背景下使用 Pytorch 进行了简单实现，这两个简单的回归基于第三章中介绍的 <strong>仿射变换</strong>，即一个带有偏置项的线性变换，但是，在实际生活中，<strong>线性</strong> 是一个非常强的假设。</p><p>我们或许有理由说一个人的年收入与其贷款是否违约具有负向线性相关性，但对于第三章章讨论的图像分类问题，就很难认为某个像素点的强度与其类别之间的关系仍是线性的。因此，我们选择构建一个深度神经网络，通过 <strong>隐藏层</strong> 的计算为我们的数据构建一种 <strong>表示</strong>，这种表示可以考虑特征之间的交互作用，在表示上，我们再建立一个线性模型用于预测可能是合适的。</p><p>通过在网络中加入一个或多个隐藏层，配合激活函数，我们便可以克服线性模型的限制，使其能处理更普遍的函数关系。最简单的方式就是将许多全连接层堆叠在一起，每一层都输出到其上面的层，直到生成最后的输出。我们可以把前 $L-1$ 层看作表示，最后一层看作线性预测器。这种架构通常称为 <strong>多层感知机</strong> (multilayer perceptron)，通常缩写为 <code>MLP</code>。一般多层感知机的架构如下图所示：</p><img src=/posts/dltorch/ch4/images/MLP.png alt=多层感知机 class=center><div style=margin-top:rem></div><p>这个多层感知机有 4 个输入，3 个输出，其隐藏层包含 5 个隐藏单元。输入层不涉及任何计算，因此，这个多层感知机中的层数为 2。由于隐藏层和输出层都是全连接的，每个输入都会影响隐藏层中的每个神经元，而隐藏层中的每个神经元又会影响输出层中的每个神经元。</p><p>以 $\bold{X}\in\mathbb{R}^{n\times d}$ 来表示 $n$ 个样本的小批量，其中每个样本具有 $d$ 个输入特征。对于具有 $h$ 个隐藏单元的单隐藏层多层感知机，用 $\bold{H}\in\mathbb{R}^{n\times h}$ 表示隐藏层的输出，称为 <strong>隐藏表示</strong> (hidden representations)，<strong>隐藏层变量</strong> (hidden-layer variable) 或 <strong>隐藏变量</strong> (hidden variable)。对于全连接的隐藏层和输出层，有隐藏层权重 $\bold{W}^{(1)}\in\mathbb{R}^{d\times h}$ 和隐藏层偏置 $\bold{b}^{(1)}\in\mathbb{R}^{1\times h}$ 以及输出层权重 $\bold{W}^{(2)}\in\mathbb{R}^{h\times q}$ 和输出层偏置 $\bold{b}^{(2)}\in\mathbb{R}^{1\times 1}$。由此便可以计算单隐藏层多层感知机的输出：
$$
\begin{align*}
\bold{H} &= \bold{XW}^{(1)} + \bold{b}^{(1)} \cr
\bold{O} &= \bold{HW}^{(2)} + \bold{b}^{(2)} \cr
\end{align*}
$$</p></p></a><div class=tags><ul style=padding-left:0><li class=rounded><a href=/cn/tags/deeplearning/ class="btn btn-sm btn-info">DeepLearning</a></li><li class=rounded><a href=/cn/tags/pytorch/ class="btn btn-sm btn-info">Pytorch</a></li><li class=rounded><a href=/cn/tags/mlp/ class="btn btn-sm btn-info">MLP</a></li><li class=rounded><a href=/cn/tags/normalization/ class="btn btn-sm btn-info">Normalization</a></li></ul></div></div><div class=card-footer><span class=float-start>Tuesday, August 13, 2024
| 2 minutes </span><a href=/cn/posts/dltorch/ch4/ class="float-end btn btn-outline-info btn-sm">阅读</a></div></div></div><div class=post-card><div class=card><div class=card-head><a href=/cn/posts/dltorch/ch3/ class=post-card-link><img class=card-img-top src alt="Hero Image"></a></div><div class=card-body><a href=/cn/posts/dltorch/ch3/ class=post-card-link><h5 class=card-title>第三章 线性神经网络</h5><p class="card-text post-summary"><h2 id=1-线性回归>1. 线性回归</h2><h3 id=11-线性回归的基本元素>1.1 线性回归的基本元素</h3><h4 id=111-线性模型>1.1.1 线性模型</h4><p>线性回归，假设自变量 $\bold{x}$ 和因变量 $y$ 之间为线性关系，其中可能包含噪声，但噪声是比较正常的，如噪声服从正态分布。</p><p>给定一个样本 $\bold{x}\in\mathbb{R}^{d}$，即具有 $d$ 个特征，将所有系数记为 $\bold{w}\in\mathbb{R}^{d}$，线性回归的基本形式为：
$$
\hat{y} = \bold{w}^{T}\bold{x} + b
$$</p><p>矩阵形式下，$\bold{X}\in\mathbb{R}^{n\times d}$ 为所有样本的特征，此时线性回归表示为：
$$
\hat{\bold{y}} = \bold{Xw} + b
$$</p><p>给定训练数据集 $\bold{X}$ 和对应标签 $\bold{y}$，线性回归的目标就是找到一组权重向量 $\bold{w}$ 和偏置 $b$，使得所有样本的预测误差尽可能小。</p><h4 id=112-损失函数>1.1.2 损失函数</h4><p>损失函数，用以度量上面提到的 “预测误差”，通常选择一个非负数作为损失，且该损失越小越好。回归问题中，最常用的损失函数为 <strong>平方误差</strong>，当样本 $i$ 的预测值为 $\hat{y}^{(i)}$，相应真实标签为 $y^{(i)}$ 时，平方误差定义为：
$$
l^{(i)}(\bold{w}, b) = \frac{1}{2}\left(\hat{y}^{(i)} - y^{(i)} \right)^{2}
$$</p><p>$\frac{1}{2}$ 是为了损失函数求导时常数系数为 1,不会有本质差别。</p><p>那么，为了度量模型在整个训练集上的表现，就需要计算在整个训练集 $n$ 个样本上的损失均值 (等价于求和)：
$$
L(\bold{w}, b) = \frac{1}{n}\sum_{i=1}^{n}l^{(i)}(\bold{w},b) = \frac{1}{n}\sum_{i=1}^{n}\frac{1}{2}\left(\bold{w}^{T}\bold{x}^{(i)} + b - y^{(i)} \right)^{2}
$$</p></p></a><div class=tags><ul style=padding-left:0><li class=rounded><a href=/cn/tags/deeplearning/ class="btn btn-sm btn-info">DeepLearning</a></li><li class=rounded><a href=/cn/tags/pytorch/ class="btn btn-sm btn-info">Pytorch</a></li><li class=rounded><a href=/cn/tags/linearregression/ class="btn btn-sm btn-info">LinearRegression</a></li><li class=rounded><a href=/cn/tags/softmax/ class="btn btn-sm btn-info">SoftMax</a></li></ul></div></div><div class=card-footer><span class=float-start>Sunday, July 28, 2024
| 8 minutes </span><a href=/cn/posts/dltorch/ch3/ class="float-end btn btn-outline-info btn-sm">阅读</a></div></div></div><div class=post-card><div class=card><div class=card-head><a href=/cn/posts/dltorch/ch2/ class=post-card-link><img class=card-img-top src=/images/default-hero.jpg alt="Hero Image"></a></div><div class=card-body><a href=/cn/posts/dltorch/ch2/ class=post-card-link><h5 class=card-title>第二章 预备知识</h5><p class="card-text post-summary"><p>接下来一段时间，我想自学深度学习，使用的教材为 <strong>动手学深度学习 (Pytorch 版)</strong>，该书有线上网址，且提供配套代码和相关 Python 包，详情可参见 <a href=https://zh.d2l.ai/ target=_blank rel=noopener>动手学深度学习</a>。</p><p>第一章内容，介绍了深度学习的相关背景和应用场景，以及深度学习领域常见的术语和名词，有一定机器学习经验的人或许已比较熟悉，故不再赘述，我们直接从第二章开始。</p><h2 id=1-tensor-操作和数据预处理>1. Tensor 操作和数据预处理</h2><p>深度学习中的数据以<strong>张量</strong> (tensor) 形式存储，支持 GPU 计算和 autograd 自动微分。</p><p>张量的创建、变形、运算 (按元素 / 矩阵)、广播机制、索引、切片等均与 <code>numpy.ndarray</code> 类似。</p><div class="alert success"><span><i data-feather=check-circle></i></span>
<span><strong>节省内存：
<code>Y = X + Y</code> 不是原地操作，即：<code>id(Y = X + Y) != id(Y)</code>，会分配新的内存。
使用 <code>Y[:] = X + Y</code> 或 <code>Y += X</code> 进行原地操作以避免不必要的内存分配。</strong></span></div><p>Tensor 可以与其他 Python 对象互相转换，如 <code>tensor.numpy()</code>。大小为 1 的张量可以转化为 Python 标量，使用 <code>tensor.item()</code> 或 <code>float(tensor)</code> 等。</p><p>数据需要经过预处理，如填充 <code>nan</code>，标准化等，可以借用其他 Python 包处理后再转化为 tensor。</p><h2 id=2-线性代数>2. 线性代数</h2><ol><li>标量，以小写字母 $x,y,z$ 等表示。</li><li>向量，以粗体小写字母 $\bold{x,y,z}$ 表示，向量的维度 (形状) 代表元素个数 (向量长度)，可以使用 <code>len(x), x.shape</code> 获取。以列向量为默认的向量方向，例如：
$$
\begin{equation*}
x = \begin{bmatrix*}
x_{1} \cr
x_{2} \cr
\vdots \cr
x_{n}
\end{bmatrix*}
\end{equation*}
$$</li><li>矩阵，以粗体大写字母 $\bold{X,Y,Z}$ 表示，是具有两个轴的张量。</li><li>张量 (此处指代数对象)，矩阵的拓展，一种具有更多轴的数据结构，使用特殊字体的大写字母 $X, Y, Z$ 表示。</li></ol><p>张量的计算，与 <code>numpy.ndarray</code> 相同，普通的加减乘除、求和、平均、向量点积、矩阵 hadamard 积、矩阵-向量积、矩阵乘法、范数等。</p></p></a><div class=tags><ul style=padding-left:0><li class=rounded><a href=/cn/tags/deeplearning/ class="btn btn-sm btn-info">DeepLearning</a></li><li class=rounded><a href=/cn/tags/pytorch/ class="btn btn-sm btn-info">Pytorch</a></li></ul></div></div><div class=card-footer><span class=float-start>Saturday, July 27, 2024
| 3 minutes </span><a href=/cn/posts/dltorch/ch2/ class="float-end btn btn-outline-info btn-sm">阅读</a></div></div></div><div class=post-card><div class=card><div class=card-head><a href=/cn/posts/quant/multi-factors/perf-attri/ class=post-card-link><img class=card-img-top src=/posts/quant/multi-factors/perf-attri/images/boat.jpg alt="Hero Image"></a></div><div class=card-body><a href=/cn/posts/quant/multi-factors/perf-attri/ class=post-card-link><h5 class=card-title>多因子绩效归因</h5><p class="card-text post-summary"><h2 id=1-摘要>1. 摘要</h2><p>投资组合的业绩归因可以分为 <strong>收益归因</strong> 和 <strong>风险归因</strong> 两个部分，而归因又可以基于净值或持仓进行。本文主要基于 <strong>持仓数据</strong> 对组合的业绩归因进行探讨。</p><p>在组合收益归因方面，主要有以下部分：</p><ul><li>基于 Brinson 模型<ul><li><strong>经典版 BHB (Brinson, Hood and Beebower) 模型</strong>：将组合超额收益分解为配置收益、选股收益和交互收益 3 部分。</li><li><strong>改进版 BF (Brinson and Fachler) 模型</strong>：引入行业超额收益，将组合超额收益分解为配置效应和选股效应两个部分。</li></ul></li><li>基于多因子模型<ul><li><strong>基于行业的多因子收益归因</strong>：与自下而上的 Brinson 模型完全一致。</li><li><strong>基于行业和风格的多因子收益归因</strong>：同时对行业和风格上的配置进行分析。</li></ul></li></ul><p>在组合风险归因方面，主要基于多因子模型：</p><ul><li><strong>单一波动分解法</strong>：单独考虑每个因子，计算简单，但忽略因子之间的协同影响，且不具可加性。</li><li><strong>边际风险分解法</strong>：将组合风险分解为因子暴露度与因子边际风险贡献的乘积，然而偏导数的概念相对模糊，指导意义不强。</li><li><strong>三要素分解法</strong>：将风险分解为因子暴露 ($x$)、因子波动 ($\sigma$) 和因子-组合相关系数 ($\rho$)，对风险的分解更为透彻，更有利于投资经理对风险进行控制。</li></ul><h2 id=2-基于-brinson-模型的组合收益归因>2. 基于 Brinson 模型的组合收益归因</h2><h3 id=21-经典-bhb-模型>2.1 经典 BHB 模型</h3><p>BHB 模型将投资组合的超额收益率分解为 <strong>配置收益、选股收益和交互收益</strong> 三个部分，其基本框架如下图所示，其中红色渲染部分表示投资组合的超额收益。</p><img src=/posts/quant/multi-factors/perf-attri/images/BHB.png alt=BNB class=center><div style=margin-top:rem></div><p>从行业配置的角度而言，假设 $w_{i}^{P}, w_{i}^{B}$ 分别表示投资组合和基准组合中行业 $i$ 的权重，$r_{i}^{P}, r_{i}^{B}$ 分别表示投资组合和基准组合中行业 $i$ 的收益率，那么投资组合的收益率 $R^{P}$ 和基准组合的收益率 $R^{B}$ 就可以表示为：
$$
\begin{align*}
R^{P}&=\sum_{i=1}^{I}w_{i}^{P}r_{i}^{P}, where\sum_{i=1}^{I}w_{i}^{P}=1 \cr
R^{B}&=\sum_{i=1}^{I}w_{i}^{B}r_{i}^{B}, where\sum_{i=1}^{I}w_{i}^{B}=1
\end{align*}
$$</p></p></a><div class=tags><ul style=padding-left:0><li class=rounded><a href=/cn/tags/%E7%BB%A9%E6%95%88%E5%BD%92%E5%9B%A0/ class="btn btn-sm btn-info">绩效归因</a></li><li class=rounded><a href=/cn/tags/%E5%A4%9A%E5%9B%A0%E5%AD%90%E6%A8%A1%E5%9E%8B/ class="btn btn-sm btn-info">多因子模型</a></li></ul></div></div><div class=card-footer><span class=float-start>Tuesday, July 23, 2024
| 2 minutes </span><a href=/cn/posts/quant/multi-factors/perf-attri/ class="float-end btn btn-outline-info btn-sm">阅读</a></div></div></div></div><div class=paginator></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-start"><div class="col-md-4 col-sm-12"><h5>导航</h5><ul><li class=nav-item><a class=smooth-scroll href=https://online727.github.io/cn/#about>关于</a></li><li class=nav-item><a class=smooth-scroll href=https://online727.github.io/cn/#skills>技能</a></li><li class=nav-item><a class=smooth-scroll href=https://online727.github.io/cn/#experiences>经历</a></li><li class=nav-item><a class=smooth-scroll href=https://online727.github.io/cn/#education>教育</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>联系方式:</h5><ul><li><a href=mailto:zhhohoh27@outlook.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>zhhohoh27@outlook.com</span></a></li><li><a href=https://github.com/online727 target=_blank rel=noopener><span><i class="fab fa-github"></i></span> <span>online727</span></a></li><li><a href=https://www.linkedin.com/in/haohan-zhao-9a6b24296 target=_blank rel=noopener><span><i class="fab fa-linkedin"></i></span> <span>赵浩翰</span></a></li><li><span><i class="fas fa-phone-alt"></i></span> <span>+8619551998168</span></li></ul></div></div></div><hr><div class=container><p id=disclaimer><strong>免责声明:</strong> 本网站基于 hugo 和 github pages 搭建，使用 hugo-toha 主题。 网站用于个人博客，所有内容均为本人所有，且不构成任何相关建议，如有问题，可与本人联系。</p></div><hr><div class=container><div class="row text-start"><div class=col-md-4><a id=theme href=https://github.com/hugo-toha/toha target=_blank rel=noopener><img src=/images/theme-logo_hu16779671404603505019.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 版权.</div><div class="col-md-4 text-end"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/application.100f2124055918d37ed55689f8b8cd9378a4ece97b585240e75bfd4894e0fe97.js integrity="sha256-EA8hJAVZGNN+1VaJ+LjNk3ik7Ol7WFJA51v9SJTg/pc=" defer></script></body></html>