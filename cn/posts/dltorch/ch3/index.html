<!doctype html><html lang=cn><head><title>第三章 线性神经网络</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=/application.d60808555b3afbcfaf18c23a60e08a98667faa5a60879f6703a355b38eafb832.css integrity="sha256-1ggIVVs6+8+vGMI6YOCKmGZ/qlpgh59nA6NVs46vuDI="><link rel=icon type=image/png href=/images/site/favicon_hu8414222332455362891.png><meta property="og:url" content="https://online727.github.io/cn/posts/dltorch/ch3/"><meta property="og:site_name" content="赵浩翰 - 博客"><meta property="og:title" content="第三章 线性神经网络"><meta property="og:description" content="动手深度学习 (Pytorch) 第三章"><meta property="og:locale" content="cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-07-28T21:57:00+08:00"><meta property="article:modified_time" content="2024-07-28T21:57:00+08:00"><meta property="article:tag" content="DeepLearning"><meta property="article:tag" content="Pytorch"><meta property="article:tag" content="LinearRegression"><meta property="article:tag" content="SoftMax"><meta name=twitter:card content="summary"><meta name=twitter:title content="第三章 线性神经网络"><meta name=twitter:description content="动手深度学习 (Pytorch) 第三章"><meta name=description content="动手深度学习 (Pytorch) 第三章"><script>theme=localStorage.getItem("theme-scheme")||localStorage.getItem("darkmode:color-scheme")||"light",theme=="system"&&(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?theme="dark":theme="light"),document.documentElement.setAttribute("data-theme",theme)</script></head><body class="type-posts kind-page" data-bs-spy=scroll data-bs-target=#TableOfContents data-bs-offset=80><div class="container-fluid bg-secondary wrapper"><nav class="navbar navbar-expand-xl top-navbar shadow" id=top-navbar><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button>
<i data-feather=sidebar></i>
</button>
<a class=navbar-brand href=/cn><img src=/images/site/main-logo_hu10708377409321002774.png id=logo alt=Logo>
赵浩翰 - 博客</a>
<button class="navbar-toggler navbar-light" id=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#top-nav-items aria-label=menu>
<i data-feather=menu></i></button><div class="collapse navbar-collapse dynamic-navbar" id=top-nav-items><ul class="nav navbar-nav ms-auto"><li class=nav-item><a class=nav-link href=/cn#home>主页</a></li><li class=nav-item><a class=nav-link href=/cn#about>关于</a></li><li class=nav-item><a class=nav-link href=/cn#skills>技能</a></li><li class=nav-item><a class=nav-link href=/cn#experiences>经历</a></li><li class=nav-item><a class=nav-link href=/cn#education>教育</a></li><div id=top-navbar-divider></div><li class=nav-item><a class=nav-link id=blog-link href=/cn/posts>博文</a></li><li class=nav-item><a class=nav-link id=note-link href=/cn/notes>笔记</a></li><li class=nav-item><a class=nav-link href=https://toha-guides.netlify.app/posts/>文档</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=languageSelector role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false><span class="fi fi-cn"></span>
简体中文</a><div class=dropdown-menu aria-labelledby=languageSelector><a class="dropdown-item nav-link languages-item" href=/posts/dltorch/ch3><span class="fi fi-gb"></span>
English
</a><a class="dropdown-item nav-link languages-item" href=/cn/posts/dltorch/ch3><span class="fi fi-cn"></span>
简体中文
</a><a class="dropdown-item nav-link languages-item" href=/bn/posts/dltorch/ch3><span class="fi fi-bd"></span>
বাংলা</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg class=theme-icon src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme"></a><div id=themeMenu class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# data-scheme=light><img class=theme-icon src=/icons/sun-svgrepo-com.svg width=20 alt="Light Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=dark><img class=theme-icon src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=system><img class=theme-icon src=/icons/computer-svgrepo-com.svg width=20 alt="System Theme"></a></div></li></ul></div></div><img src=/images/site/main-logo_hu10708377409321002774.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu8414222332455362891.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/cn/search><input type=text name=keyword placeholder=搜索 data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/cn/posts/ data-filter=all>博文</a></li><div class=subtree><li><i data-feather=plus-circle></i><a class=list-link href=/cn/posts/quant/> 量化</a><ul><li><i data-feather=plus-circle></i><a class=list-link href=/cn/posts/quant/multi-factors/> 多因子模型</a><ul><li><a class=list-link href=/cn/posts/quant/multi-factors/perf-attri/ title=多因子绩效归因>多因子绩效归因</a></li></ul></li></ul></li><li><i data-feather=minus-circle></i><a class="active list-link" href=/cn/posts/dltorch/> 深度学习 (Pytorch 版)</a><ul class=active><li><a class=list-link href=/cn/posts/dltorch/ch2/ title="第二章 预备知识">第二章 预备知识</a></li><li><a class="active list-link" href=/cn/posts/dltorch/ch3/ title="第三章 线性神经网络">第三章 线性神经网络</a></li><li><a class=list-link href=/cn/posts/dltorch/ch4/ title="第四章 多层感知机">第四章 多层感知机</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ms-auto align-self-lg-center"><img class=rounded-circle src=/images/author/zhh_hu2277249603137709564.png alt="Author Image"><h5 class=author-name>赵浩翰</h5><p class=text-muted>Sunday, July 28, 2024 | 8 minutes</p></div><div class=title><h1>第三章 线性神经网络</h1></div><div class=tags><ul style=padding-left:0><li class=rounded><a href=/cn/tags/deeplearning/ class="btn btn-sm btn-info">DeepLearning</a></li><li class=rounded><a href=/cn/tags/pytorch/ class="btn btn-sm btn-info">Pytorch</a></li><li class=rounded><a href=/cn/tags/linearregression/ class="btn btn-sm btn-info">LinearRegression</a></li><li class=rounded><a href=/cn/tags/softmax/ class="btn btn-sm btn-info">SoftMax</a></li></ul></div><div class=post-content id=post-content><h2 id=1-线性回归>1. 线性回归</h2><h3 id=11-线性回归的基本元素>1.1 线性回归的基本元素</h3><h4 id=111-线性模型>1.1.1 线性模型</h4><p>线性回归，假设自变量 $\bold{x}$ 和因变量 $y$ 之间为线性关系，其中可能包含噪声，但噪声是比较正常的，如噪声服从正态分布。</p><p>给定一个样本 $\bold{x}\in\mathbb{R}^{d}$，即具有 $d$ 个特征，将所有系数记为 $\bold{w}\in\mathbb{R}^{d}$，线性回归的基本形式为：
$$
\hat{y} = \bold{w}^{T}\bold{x} + b
$$</p><p>矩阵形式下，$\bold{X}\in\mathbb{R}^{n\times d}$ 为所有样本的特征，此时线性回归表示为：
$$
\hat{\bold{y}} = \bold{Xw} + b
$$</p><p>给定训练数据集 $\bold{X}$ 和对应标签 $\bold{y}$，线性回归的目标就是找到一组权重向量 $\bold{w}$ 和偏置 $b$，使得所有样本的预测误差尽可能小。</p><h4 id=112-损失函数>1.1.2 损失函数</h4><p>损失函数，用以度量上面提到的 “预测误差”，通常选择一个非负数作为损失，且该损失越小越好。回归问题中，最常用的损失函数为 <strong>平方误差</strong>，当样本 $i$ 的预测值为 $\hat{y}^{(i)}$，相应真实标签为 $y^{(i)}$ 时，平方误差定义为：
$$
l^{(i)}(\bold{w}, b) = \frac{1}{2}\left(\hat{y}^{(i)} - y^{(i)} \right)^{2}
$$</p><p>$\frac{1}{2}$ 是为了损失函数求导时常数系数为 1,不会有本质差别。</p><p>那么，为了度量模型在整个训练集上的表现，就需要计算在整个训练集 $n$ 个样本上的损失均值 (等价于求和)：
$$
L(\bold{w}, b) = \frac{1}{n}\sum_{i=1}^{n}l^{(i)}(\bold{w},b) = \frac{1}{n}\sum_{i=1}^{n}\frac{1}{2}\left(\bold{w}^{T}\bold{x}^{(i)} + b - y^{(i)} \right)^{2}
$$</p><p>此时，模型训练的目标就是寻找一组参数 $(\bold{w}^{*},b^{*})$，以最小化所有训练样本上的总损失，即：
$$
\bold{w}^{*},b^{*} = \argmin_{\bold{w},b}L(\bold{w},b)
$$</p><h4 id=113-解析解>1.1.3 解析解</h4><p>线性回归可以求出解析解，将偏置 $b$ 合并到权重 $\bold{w}$ 中，最小二乘法，即可得到：
$$
\bold{w}^{*} = (\bold{X}^{T}\bold{X})^{-1}\bold{X}^{T}\bold{y}
$$</p><h4 id=114-随机梯度下降>1.1.4 随机梯度下降</h4><p>对于其他更复杂的模型，可能不存在解析解，那么就需要使用一些数值优化方法，以求得数值解。深度学习中常用 <strong>梯度下降法 (Gradient Decent)</strong>。梯度下降通过计算损失函数关于模型参数的导数 (此处也可称为梯度)，来更新参数。在实际中遍历整个数据集可能非常缓慢，所以我们通常每次随机抽取一小批样本计算，这种方法称为 <strong>小批量随机梯度下降 (minibatch stochastic gradient decent)</strong>。</p><p>每次迭代，随机抽取一个小批量 $B$，计算该批次的损失均值关于参数的导数，乘以一个预先确定的正数 $\eta$ (学习率)，并从当前参数中减去，以数学公式表示如下：
$$
(\bold{w}, b)\leftarrow(\bold{w}, b) - \frac{\eta}{|B|}\sum_{i\in B}\partial_{\bold{w}, b}l^{(i)}(\bold{w}, b)
$$</p><p>总结：算法步骤如下：</p><ol><li>初始化模型参数，如随机初始化</li><li>从数据集抽取小批量样本且在负梯度方向上更新参数，并不断迭代这个步骤</li></ol><p>对于平方损失函数，我们有：
$$
\begin{align*}
\bold{w}&\leftarrow \bold{w} - \frac{\eta}{|B|}\sum_{i\in B}\partial_{\bold{w}}l^{(i)}(\bold{w}, b) = \bold{w} - \frac{\eta}{|B|}\sum_{i\in B}\bold{x}^{(i)}(\bold{w}^{T}\bold{x}^{(i)} + b - y^{(i)}) \cr
b&\leftarrow b - \frac{\eta}{|B|}\sum_{i\in B}\partial_{b}l^{(i)}(\bold{w}, b) = b - \frac{\eta}{|B|}\sum_{i\in B}(\bold{w}^{T}\bold{x}^{(i)} + b - y^{(i)})
\end{align*}
$$</p><p><strong>批量大小</strong> $B$ (batch size) 和<strong>学习率</strong> $\eta$ (learning rate) 通常是预先确定的，此类参数称为<strong>超参数</strong> (hyperparameter)，调参 (hyperparameter tuning) 就是选择超参数的过程。这个选择过程通常是根据训练迭代的结果来调整的，训练迭代结果一般在独立的 <strong>验证数据集</strong> (validation dataset) 上得到。</p><p>我们的最终目标是：通过训练集的训练和验证集上参数的选择，找到一组具有比较良好泛化 (generalization) 能力的模型参数的估计值 $\hat{\bold{w}},\hat{b}$，使其在没有见过的样本上也具有较小的损失。</p><h3 id=12-正态分布与平方损失>1.2 正态分布与平方损失</h3><p>线性回归中假设观测中包含噪声，而该噪声服从<strong>正态分布</strong>，这也是为什么线性回归可以使用均方误差的原因。噪声正态分布如下式：
$$
y = \bold{w}^{T}\bold{x}^{(i)} + b + \epsilon, \epsilon\sim N(0, \sigma^{2})
$$</p><p>下面证明为什么可以使用均方损失。给定 $\bold{x}$ 时 观测到 $y$ 的似然 (likelihood) 为：
$$
P(y|\bold{x}) = \frac{1}{\sqrt{2\pi\sigma^{2}}}exp\left(-\frac{1}{2\sigma^{2}}(y - \bold{w}^{T}\bold{x}^{(i)} - b)^{2} \right)
$$</p><p>利用<strong>极大似然估计</strong>，参数 $\bold{w}, b$ 的最优值是使整个数据集的似然最大的值，即：
$$
P(\bold{y}|\bold{X}) = \prod_{i=1}^{n}p(y^{(i)}|\bold{x}^{(i)})
$$</p><p>极大似然估计法得到的估计量称为<strong>极大似然估计量</strong>，取对数，再取负，则可以将目标变为<strong>最小化负对数似然</strong> $-\log P(\bold{y}|\bold{X})$，即：
$$
-\log P(\bold{y}|\bold{X}) = \sum_{i=1}^{n}\frac{1}{2}\log(2\pi\sigma^{2}) + \frac{1}{2\sigma^{2}}(y^{(i)} - \bold{w}^{T}\bold{x}^{(i)} - b)^{2}
$$</p><p>在正态噪声的假设下，再假设 $\sigma$ 为常数，上式即与均方误差等价。</p><h2 id=2-从零开始实现线性回归>2. 从零开始实现线性回归</h2><ol><li>生成数据集</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>synthetic_data</span>(w, b, num_examples):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;生成 y = Xw + b + 噪声&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    X <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>normal(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, (num_examples, len(w)))
</span></span><span style=display:flex><span>    y <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>matmul(X, w) <span style=color:#f92672>+</span> b
</span></span><span style=display:flex><span>    y <span style=color:#f92672>+=</span> torch<span style=color:#f92672>.</span>normal(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0.01</span>, y<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> X, y<span style=color:#f92672>.</span>reshape((<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>true_w <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>2</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>3.4</span>])
</span></span><span style=display:flex><span>true_b <span style=color:#f92672>=</span> <span style=color:#ae81ff>4.2</span>
</span></span><span style=display:flex><span>features, labels <span style=color:#f92672>=</span> synthetic_data(true_w, true_b, <span style=color:#ae81ff>1000</span>)
</span></span></code></pre></div><ol start=2><li>读取数据集，随机取一个小批量</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>data_iter</span>(batch_size, features, labels):
</span></span><span style=display:flex><span>    num_examples <span style=color:#f92672>=</span> len(features)
</span></span><span style=display:flex><span>    indices <span style=color:#f92672>=</span> list(range(num_examples))
</span></span><span style=display:flex><span>    <span style=color:#75715e># 样本随机读取，没有特定的顺序</span>
</span></span><span style=display:flex><span>    random<span style=color:#f92672>.</span>shuffle(indices)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>0</span>, num_examples, batch_size):
</span></span><span style=display:flex><span>        batch_indices <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor(
</span></span><span style=display:flex><span>            indices[i: min(i <span style=color:#f92672>+</span> batch_size, num_examples)])
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>yield</span> features[batch_indices], labels[batch_indices]
</span></span></code></pre></div><ol start=3><li>初始化模型参数，正态分布初始化</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>w <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>normal(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0.01</span>, size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>2</span>,<span style=color:#ae81ff>1</span>), requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>b <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros(<span style=color:#ae81ff>1</span>, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><ol start=4><li>定义模型</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>linreg</span>(X, w, b):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;线性回归模型&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> torch<span style=color:#f92672>.</span>matmul(X, w) <span style=color:#f92672>+</span> b
</span></span></code></pre></div><ol start=5><li>定义损失函数</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>squared_loss</span>(y_hat, y):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;均方损失&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> (y_hat <span style=color:#f92672>-</span> y<span style=color:#f92672>.</span>reshape(y_hat<span style=color:#f92672>.</span>shape)) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>/</span> <span style=color:#ae81ff>2</span>
</span></span></code></pre></div><ol start=6><li>定义优化算法</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>sgd</span>(params, lr, batch_size):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;小批量随机梯度下降&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> param <span style=color:#f92672>in</span> params:
</span></span><span style=display:flex><span>            param <span style=color:#f92672>-=</span> lr <span style=color:#f92672>*</span> param<span style=color:#f92672>.</span>grad <span style=color:#f92672>/</span> batch_size
</span></span><span style=display:flex><span>            param<span style=color:#f92672>.</span>grad<span style=color:#f92672>.</span>zero_()
</span></span></code></pre></div><ol start=7><li>训练</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>lr <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.03</span>
</span></span><span style=display:flex><span>num_epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>net <span style=color:#f92672>=</span> linreg
</span></span><span style=display:flex><span>loss <span style=color:#f92672>=</span> squared_loss
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(num_epochs):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> X, y <span style=color:#f92672>in</span> data_iter(batch_size, features, labels):
</span></span><span style=display:flex><span>        l <span style=color:#f92672>=</span> loss(net(X, w, b), y)  <span style=color:#75715e># X 和 y 的小批量损失</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 因为 l 形状是 (batch_size,1)，而不是一个标量。l 中的所有元素被加到一起，</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># 并以此计算关于 [w, b] 的梯度</span>
</span></span><span style=display:flex><span>        l<span style=color:#f92672>.</span>sum()<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>        sgd([w, b], lr, batch_size)  <span style=color:#75715e># 使用参数的梯度更新参数</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        train_l <span style=color:#f92672>=</span> loss(net(features, w, b), labels)
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;epoch </span><span style=color:#e6db74>{</span>epoch <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74>, loss </span><span style=color:#e6db74>{</span>float(train_l<span style=color:#f92672>.</span>mean())<span style=color:#e6db74>:</span><span style=color:#e6db74>f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>)
</span></span></code></pre></div><h2 id=3-线性回归的简洁实现>3. 线性回归的简洁实现</h2><ol><li>生成数据集</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch.utils <span style=color:#f92672>import</span> data
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> d2l <span style=color:#f92672>import</span> torch <span style=color:#66d9ef>as</span> d2l
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>true_w <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>tensor([<span style=color:#ae81ff>2</span>, <span style=color:#f92672>-</span><span style=color:#ae81ff>3.4</span>])
</span></span><span style=display:flex><span>true_b <span style=color:#f92672>=</span> <span style=color:#ae81ff>4.2</span>
</span></span><span style=display:flex><span>features, labels <span style=color:#f92672>=</span> d2l<span style=color:#f92672>.</span>synthetic_data(true_w, true_b, <span style=color:#ae81ff>1000</span>)
</span></span></code></pre></div><ol start=2><li>读取数据集</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_array</span>(data_arrays, batch_size, is_train<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;构造一个 PyTorch 数据迭代器&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    dataset <span style=color:#f92672>=</span> data<span style=color:#f92672>.</span>TensorDataset(<span style=color:#f92672>*</span>data_arrays)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> data<span style=color:#f92672>.</span>DataLoader(dataset, batch_size, shuffle<span style=color:#f92672>=</span>is_train)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>data_iter <span style=color:#f92672>=</span> load_array((features, labels), batch_size)
</span></span></code></pre></div><ol start=3><li>定义模型
<code>net</code> 是一个 <code>Sequential</code> 类的实例。 <code>Sequential</code> 类将多个层串联在一起。 当给定输入数据时，<code>Sequential</code> 实例将数据传入到第一层， 然后将第一层的输出作为第二层的输入，以此类推。</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># nn 是神经网络的缩写</span>
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch <span style=color:#f92672>import</span> nn
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>net <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>1</span>))
</span></span></code></pre></div><ol start=4><li>初始化模型参数
在使用 <code>net</code> 之前，需要初始化模型参数。深度学习框架通常有预定义的方法来初始化参数，在这里，我们指定每个权重参数应该从均值为 0、标准差为 0.01 的正态分布中随机采样，偏置参数将初始化为零。
通过 <code>net[0]</code> 选择网络中的第一个图层，然后使用 <code>weight.data</code> 和 <code>bias.data</code> 方法访问参数，使用替换方法 <code>normal_</code> 和 <code>fill_</code> 来重写参数值。</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>net[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>weight<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>normal_(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0.01</span>)
</span></span><span style=display:flex><span>net[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>.</span>bias<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>fill_(<span style=color:#ae81ff>0</span>)
</span></span></code></pre></div><ol start=5><li>定义损失函数
计算均方误差使用的是 <code>MSELoss</code> 类，也称为平方 $L_{2}$ 范数。默认情况下，它返回所有样本损失的平均值。</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>loss <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>MSELoss()
</span></span></code></pre></div><ol start=6><li>定义优化算法
<code>PyTorch</code> 在 <code>optim</code> 模块中实现了该算法的许多变种。实例化一个 <code>SGD</code> 实例，指定优化的参数 (可通过 <code>net.parameters()</code> 从我们的模型中获得) 以及优化算法所需的超参数。小批量随机梯度下降只需要设置 <code>lr</code> 值，这里设置为 0.03。</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>trainer <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>SGD(net<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.03</span>)
</span></span></code></pre></div><ol start=7><li>训练</li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>num_epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(num_epochs):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> X, y <span style=color:#f92672>in</span> data_iter:
</span></span><span style=display:flex><span>        l <span style=color:#f92672>=</span> loss(net(X) ,y)
</span></span><span style=display:flex><span>        trainer<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>        l<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>        trainer<span style=color:#f92672>.</span>step()
</span></span><span style=display:flex><span>    l <span style=color:#f92672>=</span> loss(net(features), labels)线性模型
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;epoch </span><span style=color:#e6db74>{</span>epoch <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span><span style=color:#e6db74>}</span><span style=color:#e6db74>, loss </span><span style=color:#e6db74>{</span>l<span style=color:#e6db74>:</span><span style=color:#e6db74>f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>)
</span></span></code></pre></div><h2 id=4-softmax-回归>4. Softmax 回归</h2><p>前述内容是应用于回归预测的线性模型，除此之外，它也可以用于分类问题。</p><h3 id=41-分类问题>4.1 分类问题</h3><p>在样本特征方面，与回归类似，每个样本有一个特征向量。而在预测标签方面，如预测猫、狗、鸡，一个直接的想法是选择 ${1,2,3}$，但这会为类别赋予“顺序”信息，在类别间有一定自然顺序时，这样做是可行的，如 ${婴儿，儿童，青年，老年}$，但该类问题亦可以转化为回归问题。因此，在预测标签方面，一般使用 <strong>独热编码</strong> (one-hot encoding)。独热编码是一个具有与类别数相同个数分量的向量，类别对应的分量设为 1，其余为 0。如猫、狗、鸡可以设置为 ${(1,0,0),(0,1,0),(0,0,1)}$。</p><h3 id=42-网络架构>4.2 网络架构</h3><p>为了估计所有可能类别的条件概率，就需要一个多输出的模型，每个类别对应一个输出，即设置与类别个数相同的仿射函数。假设如上的例子中有 4 个特征，那么我们便需要 3 个 4 元回归方程，共 12 个参数、4 各偏置。如下我们为每个输入计算 3 个未规范化的预测 (logit) $o_{1},o_{2},o_{3}$：
$$
\begin{align*}
o_{1} &= x_{1}w_{11} + x_{2}w_{12} + x_{3}w_{13} + x_{4}w_{14} + b_{1} \cr
o_{2} &= x_{1}w_{21} + x_{2}w_{22} + x_{3}w_{23} + x_{4}w_{24} + b_{2} \cr
o_{3} &= x_{1}w_{31} + x_{2}w_{32} + x_{3}w_{33} + x_{4}w_{34} + b_{3}
\end{align*}
$$</p><p>仍将模型表达为矩阵形式，则有 $\bold{o=Wx+b}, W\in\mathbb{R}^{3\times4}, x\in\mathbb{R}^{4}, b\in\mathbb{R}^{3}$。</p><p>只使用一个神经层进行 softmax 回归时，输出层同时也是全连接层，其参数开销为 $O(dq)$，$d$ 是输入维度，$q$ 是输出维度，在实践中可能非常大，但有一定的方式可以把这个开销降低至 $O(dq/n)$，$n$ 为超参数，可以灵活设置，以在参数节省和模型有效性间合理权衡。</p><h3 id=43-softmax-运算>4.3 softmax 运算</h3><p>上述网络的输出是未经规范化的预测：我们没有限制它们的和为 1，也没有限制它们的值不能为负，这违背了概率公理，因此，若要将输出视为概率，我们需要保证输出非负且和为 1，且需要一个目标函数，以激励模型精准地估计概率，该属性称之为 <strong>校准</strong> (calibration)。</p><p>softmax 函数正是我们所需要的，其计算公式如下：
$$
\hat{\bold{y}} = softmax(\bold{o}), \hat{y}_{j}=\frac{\exp (o_j)}{\sum_k \exp (o_k)}
$$</p><p>该函数不会改变原有的大小次序，且可导，我们认可通过下式选择最有可能的类别：
$$
\argmax_{j}\hat{y}_{j} = \argmax_jo_j
$$</p><h3 id=44-批量样本的向量化>4.4 批量样本的向量化</h3><p>将上述内容结合批量，输入数据为 $\bold{X}\in\mathbb{R}^{n\times d}$，权重为 $\bold{W}\in\mathbb{R}^{d\times q}$，偏置为 $\bold{b}\in\mathbb{1\times q}$，则 softmax 可以写为：
$$
\begin{align*}
\bold{O}&=\bold{XW+b} \cr
\hat{\bold{Y}} &= softmax(\bold{O})
\end{align*}
$$</p><p>其中，softmax 函数按行运算。</p><h3 id=45-损失函数>4.5 损失函数</h3><p>softmax 函数的输出给出了一个向量 $\hat{\bold{y}}$，可以理解为任意给定输入 $\bold{x}$ 时每个类别的条件概率，设整个数据集 ${\bold{X,Y}}$ 有 $n$ 个样本，索引 $i$ 的特征向量和独热标签向量分别为：$\bold{x}^{(i)},\bold{y}^{(i)}$，比较估计值和真实值即有：
$$
P(\bold{Y}|\bold{X})=\prod_{i=1}^{n}P(\bold{y}^{(i)}|\bold{x}^{(i)})
$$</p><p>进行极大似然估计，最大化 $P(\bold{Y}|\bold{X})$，即最小化负对数似然：
$$
-\log P(\bold{Y}|\bold{X}) = \sum_{i=1}^{n}-\log P(\bold{y}^{(i)}|\bold{x}^{(i)}) = \sum_{i=1^{n}}l(\bold{y}^{(i)}, \hat{\bold{y}}^{(i)})
$$</p><p>其中，对于任意标签 $\bold{y}$ 和模型预测 $\hat{\bold{y}}^{(i)}$，损失函数为：
$$
l(\bold{y}^{(i)}, \hat{\bold{y}}^{(i)})=-\sum_{j=1}^{q}y_{i}\log\hat{y}_{j}
$$</p><p>上式通常称为 <strong>交叉熵损失</strong> (cross-entropy loss)。注意，$\bold{y}$ 是一个长度为 $q$ 的独热编码向量，即只有一个分量为 1，则该式仅有一项，且由于概率值不大于 1，因此取对数后不大于 0，则该损失函数永远是一个非负值，预测的概率越准确，该值越接近于 0。</p><p>将 $\hat{y}$ 的 softmax 计算代入上式，则有：
$$
\begin{align*}
l(\bold{y}^{(i)}, \hat{\bold{y}}^{(i)}) &= -\sum_{j=1}^{q}y_j\log\frac{\exp (o_j)}{\sum_{k=1}^{q}\exp (o_k)} \cr
&= \sum_{j=1}^{q}y_{j}\log\sum_{k=1}^{q}\exp(o_k) - \sum_{j=1}^{q}y_jo_j \cr
&= \log\sum_{k=1}^{q}\exp(o_k) - \sum_{j=1}^{q}y_jo_j
\end{align*}
$$</p><p>对任意为规范化的预测 $o_j$ 求导可得：
$$
\partial_{o_{j}}l(\bold{y}^{(i)}, \hat{\bold{y}}^{(i)})=\frac{\exp(o_j)}{\sum_{k=1}^{q}\exp (o_k)} - y_j = softmax(\bold{o})_j - y_j
$$</p><p>即：导数是我们 softmax 函数分配的概率与真实独热标签表示的概率之间的差。</p><p>最后，我们的模型对任意样本的特征输出每个类别的概率，一般取其中预测概率最高的类别作为输出类别。</p><h2 id=5-fashion-mnist-数据集>5. Fashion-MNIST 数据集</h2><p>Fashion-MNIST 数据集包含 10 个类别的图像，高度和宽度为 28 像素，灰度图像，通道数为 1。训练集和测试集分别包括 60000 和 10000 张图像。首先读取数据集，并定义绘图和标签转换函数。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>%</span>matplotlib inline
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> torchvision
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch.utils <span style=color:#f92672>import</span> data
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torchvision <span style=color:#f92672>import</span> transforms
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> d2l <span style=color:#f92672>import</span> torch <span style=color:#66d9ef>as</span> d2l
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>d2l<span style=color:#f92672>.</span>use_svg_display()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 通过 ToTensor 实例将图像数据从 PIL 类型变换成 32 位浮点数格式，</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 并除以 255 使得所有像素的数值均在 0～1 之间</span>
</span></span><span style=display:flex><span>trans <span style=color:#f92672>=</span> transforms<span style=color:#f92672>.</span>ToTensor()
</span></span><span style=display:flex><span>mnist_train <span style=color:#f92672>=</span> torchvision<span style=color:#f92672>.</span>datasets<span style=color:#f92672>.</span>FashionMNIST(
</span></span><span style=display:flex><span>    root<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;../data&#34;</span>, train<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, transform<span style=color:#f92672>=</span>trans, download<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>mnist_test <span style=color:#f92672>=</span> torchvision<span style=color:#f92672>.</span>datasets<span style=color:#f92672>.</span>FashionMNIST(
</span></span><span style=display:flex><span>    root<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;../data&#34;</span>, train<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, transform<span style=color:#f92672>=</span>trans, download<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_fashion_mnist_labels</span>(labels):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;返回 Fashion-MNIST 数据集的文本标签&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    text_labels <span style=color:#f92672>=</span> [<span style=color:#e6db74>&#39;t-shirt&#39;</span>, <span style=color:#e6db74>&#39;trouser&#39;</span>, <span style=color:#e6db74>&#39;pullover&#39;</span>, <span style=color:#e6db74>&#39;dress&#39;</span>, <span style=color:#e6db74>&#39;coat&#39;</span>,
</span></span><span style=display:flex><span>                   <span style=color:#e6db74>&#39;sandal&#39;</span>, <span style=color:#e6db74>&#39;shirt&#39;</span>, <span style=color:#e6db74>&#39;sneaker&#39;</span>, <span style=color:#e6db74>&#39;bag&#39;</span>, <span style=color:#e6db74>&#39;ankle boot&#39;</span>]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> [text_labels[int(i)] <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> labels]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>show_images</span>(imgs, num_rows, num_cols, titles<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>, scale<span style=color:#f92672>=</span><span style=color:#ae81ff>1.5</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;绘制图像列表&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    figsize <span style=color:#f92672>=</span> (num_cols <span style=color:#f92672>*</span> scale, num_rows <span style=color:#f92672>*</span> scale)
</span></span><span style=display:flex><span>    _, axes <span style=color:#f92672>=</span> d2l<span style=color:#f92672>.</span>plt<span style=color:#f92672>.</span>subplots(num_rows, num_cols, figsize<span style=color:#f92672>=</span>figsize)
</span></span><span style=display:flex><span>    axes <span style=color:#f92672>=</span> axes<span style=color:#f92672>.</span>flatten()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i, (ax, img) <span style=color:#f92672>in</span> enumerate(zip(axes, imgs)):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> torch<span style=color:#f92672>.</span>is_tensor(img):
</span></span><span style=display:flex><span>            <span style=color:#75715e># 图片张量</span>
</span></span><span style=display:flex><span>            ax<span style=color:#f92672>.</span>imshow(img<span style=color:#f92672>.</span>numpy())
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            <span style=color:#75715e># PIL 图片</span>
</span></span><span style=display:flex><span>            ax<span style=color:#f92672>.</span>imshow(img)
</span></span><span style=display:flex><span>        ax<span style=color:#f92672>.</span>axes<span style=color:#f92672>.</span>get_xaxis()<span style=color:#f92672>.</span>set_visible(<span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        ax<span style=color:#f92672>.</span>axes<span style=color:#f92672>.</span>get_yaxis()<span style=color:#f92672>.</span>set_visible(<span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> titles:
</span></span><span style=display:flex><span>            ax<span style=color:#f92672>.</span>set_title(titles[i])
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> axes
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_dataloader_workers</span>():
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;使用 4 个进程来读取数据&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>4</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_data_fashion_mnist</span>(batch_size, resize<span style=color:#f92672>=</span><span style=color:#66d9ef>None</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;下载 Fashion-MNIST 数据集，然后将其加载到内存中&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 通过 ToTensor 实例将图像数据从 PIL 类型变换成 32 位浮点数格式</span>
</span></span><span style=display:flex><span>    trans <span style=color:#f92672>=</span> [transforms<span style=color:#f92672>.</span>ToTensor()]
</span></span><span style=display:flex><span>    <span style=color:#75715e># 使用 resize 将图像调整到另一种形状</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> resize:
</span></span><span style=display:flex><span>        trans<span style=color:#f92672>.</span>insert(<span style=color:#ae81ff>0</span>, transforms<span style=color:#f92672>.</span>Resize(resize))
</span></span><span style=display:flex><span>    trans <span style=color:#f92672>=</span> transforms<span style=color:#f92672>.</span>Compose(trans)
</span></span><span style=display:flex><span>    <span style=color:#75715e># 下载数据集</span>
</span></span><span style=display:flex><span>    mnist_train <span style=color:#f92672>=</span> torchvision<span style=color:#f92672>.</span>datasets<span style=color:#f92672>.</span>FashionMNIST(
</span></span><span style=display:flex><span>        root<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;../data&#34;</span>, train<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>, transform<span style=color:#f92672>=</span>trans, download<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    mnist_test <span style=color:#f92672>=</span> torchvision<span style=color:#f92672>.</span>datasets<span style=color:#f92672>.</span>FashionMNIST(
</span></span><span style=display:flex><span>        root<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;../data&#34;</span>, train<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, transform<span style=color:#f92672>=</span>trans, download<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    <span style=color:#75715e># 返回内置数据迭代器</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> (data<span style=color:#f92672>.</span>DataLoader(mnist_train, batch_size, shuffle<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>                            num_workers<span style=color:#f92672>=</span>get_dataloader_workers()),
</span></span><span style=display:flex><span>            data<span style=color:#f92672>.</span>DataLoader(mnist_test, batch_size, shuffle<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>                            num_workers<span style=color:#f92672>=</span>get_dataloader_workers()))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_iter, test_iter <span style=color:#f92672>=</span> load_data_fashion_mnist(<span style=color:#ae81ff>32</span>, resize<span style=color:#f92672>=</span><span style=color:#ae81ff>64</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> X, y <span style=color:#f92672>in</span> train_iter:
</span></span><span style=display:flex><span>    print(X<span style=color:#f92672>.</span>shape, X<span style=color:#f92672>.</span>dtype, y<span style=color:#f92672>.</span>shape, y<span style=color:#f92672>.</span>dtype)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>break</span>
</span></span></code></pre></div><h2 id=6-softmax-回归从零实现>6. softmax 回归从零实现</h2><p>首先，加载数据并初始化模型参数：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> IPython <span style=color:#f92672>import</span> display
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> d2l <span style=color:#f92672>import</span> torch <span style=color:#66d9ef>as</span> d2l
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>256</span>
</span></span><span style=display:flex><span>train_iter, test_iter <span style=color:#f92672>=</span> d2l<span style=color:#f92672>.</span>load_data_fashion_mnist(batch_size)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>num_inputs <span style=color:#f92672>=</span> <span style=color:#ae81ff>784</span>    <span style=color:#75715e># 28*28 的图像视为一维向量</span>
</span></span><span style=display:flex><span>num_outputs <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>    <span style=color:#75715e># 对应 10 个类别</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>W <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>normal(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0.01</span>, size<span style=color:#f92672>=</span>(num_inputs, num_outputs), requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)   <span style=color:#75715e># 784*10</span>
</span></span><span style=display:flex><span>b <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros(num_outputs, requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)    <span style=color:#75715e># 1*10</span>
</span></span></code></pre></div><p>定义模型、softmax 函数、损失函数、分类精度、评估函数：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>softmax</span>(X):
</span></span><span style=display:flex><span>    X_exp <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>exp(X)
</span></span><span style=display:flex><span>    partition <span style=color:#f92672>=</span> X_exp<span style=color:#f92672>.</span>sum(<span style=color:#ae81ff>1</span>, keepdim<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> X_exp <span style=color:#f92672>/</span> partition
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>net</span>(X):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> softmax(torch<span style=color:#f92672>.</span>matmul(X<span style=color:#f92672>.</span>reshape(<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, W<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]), W) <span style=color:#f92672>+</span> b)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>cross_entropy</span>(y_hat, y):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#f92672>-</span> torch<span style=color:#f92672>.</span>log(y_hat[range(len(y_hat)), y])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>accuracy</span>(y_hat, y):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;计算预测正确的数量&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> (len(y_hat<span style=color:#f92672>.</span>shape) <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>and</span> (y_hat<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>1</span>):
</span></span><span style=display:flex><span>        y_hat <span style=color:#f92672>=</span> y_hat<span style=color:#f92672>.</span>argmax(axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>    cmp <span style=color:#f92672>=</span> y_hat<span style=color:#f92672>.</span>type(y<span style=color:#f92672>.</span>dtype) <span style=color:#f92672>==</span> y
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> float(cmp<span style=color:#f92672>.</span>type(y<span style=color:#f92672>.</span>dtype)<span style=color:#f92672>.</span>sum())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>evaluate_accuracy</span>(net, data_iter):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;计算在指定数据集上模型的精度&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> isinstance(net, torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>        net<span style=color:#f92672>.</span>eval()  <span style=color:#75715e># 将模型设置为评估模式</span>
</span></span><span style=display:flex><span>    metric <span style=color:#f92672>=</span> d2l<span style=color:#f92672>.</span>Accumulator(<span style=color:#ae81ff>2</span>)  <span style=color:#75715e># 正确预测数、预测总数</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> X, y <span style=color:#f92672>in</span> data_iter:
</span></span><span style=display:flex><span>            metric<span style=color:#f92672>.</span>add(accuracy(net(X), y), y<span style=color:#f92672>.</span>numel())
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> metric[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>/</span> metric[<span style=color:#ae81ff>1</span>]
</span></span></code></pre></div><p>定义训练、预测过程：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_epoch_ch3</span>(net, train_iter, loss, updater):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;训练模型一个迭代周期（定义见第 3 章）&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># 将模型设置为训练模式</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> isinstance(net, torch<span style=color:#f92672>.</span>nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>        net<span style=color:#f92672>.</span>train()
</span></span><span style=display:flex><span>    <span style=color:#75715e># 训练损失总和、训练准确度总和、样本数</span>
</span></span><span style=display:flex><span>    metric <span style=color:#f92672>=</span> Accumulator(<span style=color:#ae81ff>3</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> X, y <span style=color:#f92672>in</span> train_iter:
</span></span><span style=display:flex><span>        <span style=color:#75715e># 计算梯度并更新参数</span>
</span></span><span style=display:flex><span>        y_hat <span style=color:#f92672>=</span> net(X)
</span></span><span style=display:flex><span>        l <span style=color:#f92672>=</span> loss(y_hat, y)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> isinstance(updater, torch<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>Optimizer):
</span></span><span style=display:flex><span>            <span style=color:#75715e># 使用 PyTorch 内置的优化器和损失函数</span>
</span></span><span style=display:flex><span>            updater<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>            l<span style=color:#f92672>.</span>mean()<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>            updater<span style=color:#f92672>.</span>step()
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            <span style=color:#75715e># 使用定制的优化器和损失函数</span>
</span></span><span style=display:flex><span>            l<span style=color:#f92672>.</span>sum()<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>            updater(X<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>        metric<span style=color:#f92672>.</span>add(float(l<span style=color:#f92672>.</span>sum()), accuracy(y_hat, y), y<span style=color:#f92672>.</span>numel())
</span></span><span style=display:flex><span>    <span style=color:#75715e># 返回训练损失和训练精度</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> metric[<span style=color:#ae81ff>0</span>] <span style=color:#f92672>/</span> metric[<span style=color:#ae81ff>2</span>], metric[<span style=color:#ae81ff>1</span>] <span style=color:#f92672>/</span> metric[<span style=color:#ae81ff>2</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train_ch3</span>(net, train_iter, test_iter, loss, num_epochs, updater):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;训练模型（定义见第 3 章）&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    animator <span style=color:#f92672>=</span> d2l<span style=color:#f92672>.</span>Animator(xlabel<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;epoch&#39;</span>, xlim<span style=color:#f92672>=</span>[<span style=color:#ae81ff>1</span>, num_epochs], ylim<span style=color:#f92672>=</span>[<span style=color:#ae81ff>0.3</span>, <span style=color:#ae81ff>0.9</span>],
</span></span><span style=display:flex><span>                        legend<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;train loss&#39;</span>, <span style=color:#e6db74>&#39;train acc&#39;</span>, <span style=color:#e6db74>&#39;test acc&#39;</span>])
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(num_epochs):
</span></span><span style=display:flex><span>        train_metrics <span style=color:#f92672>=</span> train_epoch_ch3(net, train_iter, loss, updater)
</span></span><span style=display:flex><span>        test_acc <span style=color:#f92672>=</span> evaluate_accuracy(net, test_iter)
</span></span><span style=display:flex><span>        animator<span style=color:#f92672>.</span>add(epoch <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>, train_metrics <span style=color:#f92672>+</span> (test_acc,))
</span></span><span style=display:flex><span>    train_loss, train_acc <span style=color:#f92672>=</span> train_metrics
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> train_loss <span style=color:#f92672>&lt;</span> <span style=color:#ae81ff>0.5</span>, train_loss
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> train_acc <span style=color:#f92672>&lt;=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>and</span> train_acc <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0.7</span>, train_acc
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>assert</span> test_acc <span style=color:#f92672>&lt;=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>and</span> test_acc <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0.7</span>, test_acc
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>lr <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>updater</span>(batch_size):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> d2l<span style=color:#f92672>.</span>sgd([W, b], lr, batch_size)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>num_epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict_ch3</span>(net, test_iter, n<span style=color:#f92672>=</span><span style=color:#ae81ff>6</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;预测标签（定义见第 3 章）&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> X, y <span style=color:#f92672>in</span> test_iter:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>    trues <span style=color:#f92672>=</span> d2l<span style=color:#f92672>.</span>get_fashion_mnist_labels(y)
</span></span><span style=display:flex><span>    preds <span style=color:#f92672>=</span> d2l<span style=color:#f92672>.</span>get_fashion_mnist_labels(net(X)<span style=color:#f92672>.</span>argmax(axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>))
</span></span><span style=display:flex><span>    titles <span style=color:#f92672>=</span> [true <span style=color:#f92672>+</span><span style=color:#e6db74>&#39;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#39;</span> <span style=color:#f92672>+</span> pred <span style=color:#66d9ef>for</span> true, pred <span style=color:#f92672>in</span> zip(trues, preds)]
</span></span><span style=display:flex><span>    d2l<span style=color:#f92672>.</span>show_images(
</span></span><span style=display:flex><span>        X[<span style=color:#ae81ff>0</span>:n]<span style=color:#f92672>.</span>reshape((n, <span style=color:#ae81ff>28</span>, <span style=color:#ae81ff>28</span>)), <span style=color:#ae81ff>1</span>, n, titles<span style=color:#f92672>=</span>titles[<span style=color:#ae81ff>0</span>:n])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>predict_ch3(net, test_iter)
</span></span></code></pre></div><p>预测结果如下所示：</p><img src=/posts/dltorch/ch3/images/softmax1.jpg alt="simple softmax" class=center><div style=margin-top:rem></div><h2 id=7-softmax-回归的简洁实现>7. softmax 回归的简洁实现</h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> torch
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> torch <span style=color:#f92672>import</span> nn
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> d2l <span style=color:#f92672>import</span> torch <span style=color:#66d9ef>as</span> d2l
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 加载数据</span>
</span></span><span style=display:flex><span>batch_size <span style=color:#f92672>=</span> <span style=color:#ae81ff>256</span>
</span></span><span style=display:flex><span>train_iter, test_iter <span style=color:#f92672>=</span> d2l<span style=color:#f92672>.</span>load_data_fashion_mnist(batch_size)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 定义网络</span>
</span></span><span style=display:flex><span><span style=color:#75715e># PyTorch 不会隐式地调整输入的形状。因此，我们在线性层前定义了展平层（flatten），来调整网络输入的形状</span>
</span></span><span style=display:flex><span>net <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(nn<span style=color:#f92672>.</span>Flatten(), nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>784</span>, <span style=color:#ae81ff>10</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>init_weights</span>(m):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> type(m) <span style=color:#f92672>==</span> nn<span style=color:#f92672>.</span>Linear:
</span></span><span style=display:flex><span>        nn<span style=color:#f92672>.</span>init<span style=color:#f92672>.</span>normal_(m<span style=color:#f92672>.</span>weight, std<span style=color:#f92672>=</span><span style=color:#ae81ff>0.01</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>net<span style=color:#f92672>.</span>apply(init_weights)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 损失函数</span>
</span></span><span style=display:flex><span><span style=color:#75715e># PyTorch 为交叉熵损失函数传递未规范化的预测，在函数中再计算 softmax 及其对数</span>
</span></span><span style=display:flex><span>loss <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>CrossEntropyLoss(reduction<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;none&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 优化器</span>
</span></span><span style=display:flex><span>trainer <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>SGD(net<span style=color:#f92672>.</span>parameters(), lr<span style=color:#f92672>=</span><span style=color:#ae81ff>0.1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 训练</span>
</span></span><span style=display:flex><span>num_epochs <span style=color:#f92672>=</span> <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>d2l<span style=color:#f92672>.</span>train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
</span></span></code></pre></div><h3 id=71-重新审视-softmax-的实现>7.1 重新审视 softmax 的实现</h3><p>softmax 函数中需要取 $\exp$，在遇到较大的 $o_k$ 时，可能会导致数值 <strong>上溢</strong>，最终结果可能出现 0、inf 或 nan 值。</p><p>我们可以通过为所有 $o_k$ 减去 $\max(o_k)$ 来解决上溢问题，因为：
$$
\begin{align*}
\hat{y}_j &= \frac{\exp(o_j-\max(o_k))\exp(\max(o_k))}{\sum_k\exp(o_k-\max(o_k))\exp(\max(o_k))} \cr
&= \frac{\exp(o_j-\max(o_k))}{\sum_k\exp(o_k-\max(o_k))}
\end{align*}
$$</p><p>但在上述减法和规范化步骤后，可能有一些 $o_j-\max(o_k)$ 出现较大负值，取指数后的值非常接近 0，即 <strong>下溢</strong>，则最后取对数时得到 -inf 值。</p><p>但幸运的是，尽管我们要计算指数函数，但我们最终在计算交叉熵损失时会取它们的对数。 通过将 softmax 和交叉熵结合在一起，可以避免反向传播过程中可能会困扰我们的数值稳定性问题。 如下面的等式所示，我们避免计算 $\exp(o_j-\max(o_k))$，直接使用 $o_j-\max(o_k)$：
$$
\begin{align*}
\log(\hat{y}_j) &= \log\left(\frac{\exp(o_j-\max(o_k))}{\sum_k\exp(o_k-\max(o_k))}\right) \cr
&= \log(\exp(o_j-\max(o_k))) - \log\left(\sum_k\exp(o_k-\max(o_k))\right) \cr
&= o_j-\max(o_k) - \log\left(\sum_k\exp(o_k-\max(o_k))\right)
\end{align*}
$$</p><p>PyTorch 没有将 softmax 概率传递到损失函数中， 而是在交叉熵损失函数中传递未规范化的预测，在函数中再计算 softmax 及其对数， 这是一种类似 “LogSumExp 技巧” 的聪明方式。</p></div><div class="row ps-3 pe-3"><div class="col-md-6 share-buttons"><strong>分享:</strong>
<a class="btn icon-button bg-facebook" href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fonline727.github.io%2fcn%2fposts%2fdltorch%2fch3%2f" target=_blank><i class="fab fa-facebook"></i>
</a><a class="btn icon-button bg-twitter" href="https://twitter.com/share?url=https%3a%2f%2fonline727.github.io%2fcn%2fposts%2fdltorch%2fch3%2f&text=%e7%ac%ac%e4%b8%89%e7%ab%a0%20%e7%ba%bf%e6%80%a7%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c&via=%e8%b5%b5%e6%b5%a9%e7%bf%b0%20-%20%e5%8d%9a%e5%ae%a2" target=_blank><i class="fab fa-twitter"></i>
</a><a class="btn icon-button bg-reddit" href="https://reddit.com/submit?url=https%3a%2f%2fonline727.github.io%2fcn%2fposts%2fdltorch%2fch3%2f&title=%e7%ac%ac%e4%b8%89%e7%ab%a0%20%e7%ba%bf%e6%80%a7%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c" target=_blank><i class="fab fa-reddit"></i>
</a><a class="btn icon-button bg-linkedin" href="https://www.linkedin.com/shareArticle?url=https%3a%2f%2fonline727.github.io%2fcn%2fposts%2fdltorch%2fch3%2f&title=%e7%ac%ac%e4%b8%89%e7%ab%a0%20%e7%ba%bf%e6%80%a7%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c" target=_blank><i class="fab fa-linkedin"></i>
</a><a class="btn icon-button bg-whatsapp" href="https://api.whatsapp.com/send?text=%e7%ac%ac%e4%b8%89%e7%ab%a0%20%e7%ba%bf%e6%80%a7%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c https%3a%2f%2fonline727.github.io%2fcn%2fposts%2fdltorch%2fch3%2f" target=_blank><i class="fab fa-whatsapp"></i>
</a><a class="btn icon-button" href="mailto:?subject=%e7%ac%ac%e4%b8%89%e7%ab%a0%20%e7%ba%bf%e6%80%a7%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c&body=https%3a%2f%2fonline727.github.io%2fcn%2fposts%2fdltorch%2fch3%2f" target=_blank><i class="fas fa-envelope-open-text"></i></a></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/online727/online727.github.io/edit/main/content/posts/dltorch/ch3/index.cn.md title=改善此页面 target=_blank rel=noopener><i class="fas fa-code-branch"></i>
改善此页面</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/cn/posts/dltorch/ch2/ title="第二章 预备知识" class="btn filled-button"><div><i class="fas fa-chevron-circle-left"></i> 上一篇</div><div class=next-prev-text>第二章 预备知识</div></a></div><div class="col-md-6 next-article"><a href=/cn/posts/dltorch/ch4/ title="第四章 多层感知机" class="btn filled-button"><div>下一篇 <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>第四章 多层感知机</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn type=button data-bs-toggle=tooltip data-bs-placement=left title="Scroll to top"><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center ps-3">目录</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#1-线性回归>1. 线性回归</a><ul><li><a href=#11-线性回归的基本元素>1.1 线性回归的基本元素</a><ul><li><a href=#111-线性模型>1.1.1 线性模型</a></li><li><a href=#112-损失函数>1.1.2 损失函数</a></li><li><a href=#113-解析解>1.1.3 解析解</a></li><li><a href=#114-随机梯度下降>1.1.4 随机梯度下降</a></li></ul></li><li><a href=#12-正态分布与平方损失>1.2 正态分布与平方损失</a></li></ul></li><li><a href=#2-从零开始实现线性回归>2. 从零开始实现线性回归</a></li><li><a href=#3-线性回归的简洁实现>3. 线性回归的简洁实现</a></li><li><a href=#4-softmax-回归>4. Softmax 回归</a><ul><li><a href=#41-分类问题>4.1 分类问题</a></li><li><a href=#42-网络架构>4.2 网络架构</a></li><li><a href=#43-softmax-运算>4.3 softmax 运算</a></li><li><a href=#44-批量样本的向量化>4.4 批量样本的向量化</a></li><li><a href=#45-损失函数>4.5 损失函数</a></li></ul></li><li><a href=#5-fashion-mnist-数据集>5. Fashion-MNIST 数据集</a></li><li><a href=#6-softmax-回归从零实现>6. softmax 回归从零实现</a></li><li><a href=#7-softmax-回归的简洁实现>7. softmax 回归的简洁实现</a><ul><li><a href=#71-重新审视-softmax-的实现>7.1 重新审视 softmax 的实现</a></li></ul></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-start"><div class="col-md-4 col-sm-12"><h5>导航</h5><ul><li class=nav-item><a class=smooth-scroll href=https://online727.github.io/cn/#about>关于</a></li><li class=nav-item><a class=smooth-scroll href=https://online727.github.io/cn/#skills>技能</a></li><li class=nav-item><a class=smooth-scroll href=https://online727.github.io/cn/#experiences>经历</a></li><li class=nav-item><a class=smooth-scroll href=https://online727.github.io/cn/#education>教育</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>联系方式:</h5><ul><li><a href=mailto:zhhohoh27@outlook.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>zhhohoh27@outlook.com</span></a></li><li><a href=https://github.com/online727 target=_blank rel=noopener><span><i class="fab fa-github"></i></span> <span>online727</span></a></li><li><a href=https://www.linkedin.com/in/haohan-zhao-9a6b24296 target=_blank rel=noopener><span><i class="fab fa-linkedin"></i></span> <span>赵浩翰</span></a></li><li><span><i class="fas fa-phone-alt"></i></span> <span>+8619551998168</span></li></ul></div></div></div><hr><div class=container><p id=disclaimer><strong>免责声明:</strong> 本网站基于 hugo 和 github pages 搭建，使用 hugo-toha 主题。 网站用于个人博客，所有内容均为本人所有，且不构成任何相关建议，如有问题，可与本人联系。</p></div><hr><div class=container><div class="row text-start"><div class=col-md-4><a id=theme href=https://github.com/hugo-toha/toha target=_blank rel=noopener><img src=/images/theme-logo_hu16779671404603505019.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 版权.</div><div class="col-md-4 text-end"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/application.aa1b29568827d862abc07cd2a25fa9f3f1dad96fd8fe2a012bccae2de39367fc.js integrity="sha256-qhspVogn2GKrwHzSol+p8/Ha2W/Y/ioBK8yuLeOTZ/w=" defer></script></body></html>