<!doctype html><html lang=cn><head><title>第二章 预备知识</title>
<meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="ie=edge"><link rel=stylesheet href=/application.d60808555b3afbcfaf18c23a60e08a98667faa5a60879f6703a355b38eafb832.css integrity="sha256-1ggIVVs6+8+vGMI6YOCKmGZ/qlpgh59nA6NVs46vuDI="><link rel=icon type=image/png href=/images/site/favicon_hu8414222332455362891.png><meta property="og:url" content="https://online727.github.io/cn/posts/dltorch/ch2/"><meta property="og:site_name" content="赵浩翰 - 博客"><meta property="og:title" content="第二章 预备知识"><meta property="og:description" content="动手深度学习 (Pytorch) 第二章"><meta property="og:locale" content="cn"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-07-27T17:06:25+08:00"><meta property="article:modified_time" content="2024-07-27T17:06:25+08:00"><meta property="article:tag" content="DeepLearning"><meta property="article:tag" content="Pytorch"><meta name=twitter:card content="summary"><meta name=twitter:title content="第二章 预备知识"><meta name=twitter:description content="动手深度学习 (Pytorch) 第二章"><meta name=description content="动手深度学习 (Pytorch) 第二章"><script>theme=localStorage.getItem("theme-scheme")||localStorage.getItem("darkmode:color-scheme")||"light",theme=="system"&&(window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches?theme="dark":theme="light"),document.documentElement.setAttribute("data-theme",theme)</script></head><body class="type-posts kind-page" data-bs-spy=scroll data-bs-target=#TableOfContents data-bs-offset=80><div class="container-fluid bg-secondary wrapper"><nav class="navbar navbar-expand-xl top-navbar shadow" id=top-navbar><div class=container><button class="navbar-toggler navbar-light" id=sidebar-toggler type=button>
<i data-feather=sidebar></i>
</button>
<a class=navbar-brand href=/cn><img src=/images/site/main-logo_hu10708377409321002774.png id=logo alt=Logo>
赵浩翰 - 博客</a>
<button class="navbar-toggler navbar-light" id=navbar-toggler type=button data-bs-toggle=collapse data-bs-target=#top-nav-items aria-label=menu>
<i data-feather=menu></i></button><div class="collapse navbar-collapse dynamic-navbar" id=top-nav-items><ul class="nav navbar-nav ms-auto"><li class=nav-item><a class=nav-link href=/cn#home>主页</a></li><li class=nav-item><a class=nav-link href=/cn#about>关于</a></li><li class=nav-item><a class=nav-link href=/cn#education>教育</a></li><li class=nav-item><a class=nav-link href=/cn#experiences>经历</a></li><li class=nav-item><a class=nav-link href=/cn#projects>项目</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=navbarDropdown role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false>更多的</a><div class=dropdown-menu aria-labelledby=navbarDropdown><a class=dropdown-item href=/cn#skills>技能</a>
<a class=dropdown-item href=/cn#recent-posts>近期帖子</a></div></li><div id=top-navbar-divider></div><li class=nav-item><a class=nav-link id=blog-link href=/cn/posts>博文</a></li><li class=nav-item><a class=nav-link id=note-link href=/cn/notes>笔记</a></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=languageSelector role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false><span class="fi fi-cn"></span>
简体中文</a><div class=dropdown-menu aria-labelledby=languageSelector><a class="dropdown-item nav-link languages-item" href=/posts/dltorch/ch2><span class="fi fi-gb"></span>
English
</a><a class="dropdown-item nav-link languages-item" href=/cn/posts/dltorch/ch2><span class="fi fi-cn"></span>
简体中文
</a><a class="dropdown-item nav-link languages-item" href=/bn/posts/dltorch/ch2><span class="fi fi-bd"></span>
বাংলা</a></div></li><li class="nav-item dropdown"><a class="nav-link dropdown-toggle" href=# id=themeSelector role=button data-bs-toggle=dropdown aria-haspopup=true aria-expanded=false><img id=navbar-theme-icon-svg class=theme-icon src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme"></a><div id=themeMenu class="dropdown-menu dropdown-menu-icons-only" aria-labelledby=themeSelector><a class="dropdown-item nav-link" href=# data-scheme=light><img class=theme-icon src=/icons/sun-svgrepo-com.svg width=20 alt="Light Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=dark><img class=theme-icon src=/icons/moon-svgrepo-com.svg width=20 alt="Dark Theme">
</a><a class="dropdown-item nav-link" href=# data-scheme=system><img class=theme-icon src=/icons/computer-svgrepo-com.svg width=20 alt="System Theme"></a></div></li></ul></div></div><img src=/images/site/main-logo_hu10708377409321002774.png class=d-none id=main-logo alt=Logo>
<img src=/images/site/inverted-logo_hu8414222332455362891.png class=d-none id=inverted-logo alt="Inverted Logo"></nav><section class=sidebar-section id=sidebar-section><div class=sidebar-holder><div class=sidebar id=sidebar><form class=mx-auto method=get action=/cn/search><input type=text name=keyword placeholder=搜索 data-search id=search-box></form><div class=sidebar-tree><ul class=tree id=tree><li id=list-heading><a href=/cn/posts/ data-filter=all>博文</a></li><div class=subtree><li><i data-feather=plus-circle></i><a class=list-link href=/cn/posts/quant/> 量化</a><ul><li><i data-feather=plus-circle></i><a class=list-link href=/cn/posts/quant/multi-factors/> 多因子模型</a><ul><li><a class=list-link href=/cn/posts/quant/multi-factors/perf-attri/ title=多因子绩效归因>多因子绩效归因</a></li></ul></li></ul></li><li><i data-feather=minus-circle></i><a class="active list-link" href=/cn/posts/dltorch/> 深度学习 (Pytorch 版)</a><ul class=active><li><a class="active list-link" href=/cn/posts/dltorch/ch2/ title="第二章 预备知识">第二章 预备知识</a></li><li><a class=list-link href=/cn/posts/dltorch/ch3/ title="第三章 线性神经网络">第三章 线性神经网络</a></li><li><a class=list-link href=/cn/posts/dltorch/ch4/ title="第四章 多层感知机">第四章 多层感知机</a></li></ul></li></div></ul></div></div></div></section><section class=content-section id=content-section><div class=content><div class="container p-0 read-area"><div class="hero-area col-sm-12" id=hero-area style=background-image:url(/images/default-hero.jpg)></div><div class=page-content><div class="author-profile ms-auto align-self-lg-center"><img class=rounded-circle src=/images/author/zhh_hu2277249603137709564.png alt="Author Image"><h5 class=author-name>赵浩翰</h5><p class=text-muted>Saturday, July 27, 2024 | 3 minutes</p></div><div class=title><h1>第二章 预备知识</h1></div><div class=tags><ul style=padding-left:0><li class=rounded><a href=/cn/tags/deeplearning/ class="btn btn-sm btn-info">DeepLearning</a></li><li class=rounded><a href=/cn/tags/pytorch/ class="btn btn-sm btn-info">Pytorch</a></li></ul></div><div class=post-content id=post-content><p>接下来一段时间，我想自学深度学习，使用的教材为 <strong>动手学深度学习 (Pytorch 版)</strong>，该书有线上网址，且提供配套代码和相关 Python 包，详情可参见 <a href=https://zh.d2l.ai/ target=_blank rel=noopener>动手学深度学习</a>。</p><p>第一章内容，介绍了深度学习的相关背景和应用场景，以及深度学习领域常见的术语和名词，有一定机器学习经验的人或许已比较熟悉，故不再赘述，我们直接从第二章开始。</p><h2 id=1-tensor-操作和数据预处理>1. Tensor 操作和数据预处理</h2><p>深度学习中的数据以<strong>张量</strong> (tensor) 形式存储，支持 GPU 计算和 autograd 自动微分。</p><p>张量的创建、变形、运算 (按元素 / 矩阵)、广播机制、索引、切片等均与 <code>numpy.ndarray</code> 类似。</p><div class="alert success"><span><i data-feather=check-circle></i></span>
<span><strong>节省内存：
<code>Y = X + Y</code> 不是原地操作，即：<code>id(Y = X + Y) != id(Y)</code>，会分配新的内存。
使用 <code>Y[:] = X + Y</code> 或 <code>Y += X</code> 进行原地操作以避免不必要的内存分配。</strong></span></div><p>Tensor 可以与其他 Python 对象互相转换，如 <code>tensor.numpy()</code>。大小为 1 的张量可以转化为 Python 标量，使用 <code>tensor.item()</code> 或 <code>float(tensor)</code> 等。</p><p>数据需要经过预处理，如填充 <code>nan</code>，标准化等，可以借用其他 Python 包处理后再转化为 tensor。</p><h2 id=2-线性代数>2. 线性代数</h2><ol><li>标量，以小写字母 $x,y,z$ 等表示。</li><li>向量，以粗体小写字母 $\bold{x,y,z}$ 表示，向量的维度 (形状) 代表元素个数 (向量长度)，可以使用 <code>len(x), x.shape</code> 获取。以列向量为默认的向量方向，例如：
$$
\begin{equation*}
x = \begin{bmatrix*}
x_{1} \cr
x_{2} \cr
\vdots \cr
x_{n}
\end{bmatrix*}
\end{equation*}
$$</li><li>矩阵，以粗体大写字母 $\bold{X,Y,Z}$ 表示，是具有两个轴的张量。</li><li>张量 (此处指代数对象)，矩阵的拓展，一种具有更多轴的数据结构，使用特殊字体的大写字母 $X, Y, Z$ 表示。</li></ol><p>张量的计算，与 <code>numpy.ndarray</code> 相同，普通的加减乘除、求和、平均、向量点积、矩阵 hadamard 积、矩阵-向量积、矩阵乘法、范数等。</p><h2 id=3-微积分>3. 微积分</h2><h3 id=31-导数和微分>3.1 导数和微分</h3><p>设 $y=f(x)$，其导数被定义为：
$$
f^{&rsquo;}(x) = \lim_{h\rightarrow 0}\frac{f(x+h) - f(x)}{h}
$$</p><p>以下符号等价：
$$
f^{&rsquo;}(x)=y^{&rsquo;}=\frac{dy}{dx}=\frac{df}{dx}=\frac{d}{dx}f(x)=Df(x)=D_{x}f(x)
$$</p><p>自行回顾求导法则。</p><h3 id=32-偏导数>3.2 偏导数</h3><p>将微分的思想拓展至多元函数上，设 $y=f(x_{1},\cdots,x_{n})$ 是一个 $n$ 元函数，其关于第 $i$ 个变量 $x_{i}$ 的偏导数为：
$$
\frac{\partial y}{\partial x_{i}} = \lim_{h\rightarrow 0}\frac{f(x_{1},\cdots,x_{i-1},x_{i}+h,x_{i+1},\cdots,x_{n}) - f(x_{1},\cdots,x_{i},\cdot,x_{n})}{h}
$$</p><p>以下表达等价：
$$
\frac{\partial y}{\partial x_{i}} = \frac{\partial f}{\partial x_{i}} = f_{x_{i}} = f_{i} = D_{i}f = D_{x_{i}}f
$$</p><h3 id=33-梯度>3.3 梯度</h3><p>函数的梯度（gradient）向量，即其对所有变量的偏导数。设函数 $f: \mathbb{R}^n \to \mathbb{R}$ 的输入是一个 $n$ 维向量 $\mathbf{x} = [x_1, x_2, \ldots, x_n]$，并且输出是一个标量。函数 $f(\mathbf{x})$ 相对于 $\mathbf{x}$ 的梯度是一个包含 $n$ 个偏导数的向量：
$$
\nabla_{\mathbf{x}} f(\mathbf{x}) = \begin{bmatrix}
\frac{\partial f(\mathbf{x})}{\partial x_1} & \frac{\partial f(\mathbf{x})}{\partial x_2} & \cdots & \frac{\partial f(\mathbf{x})}{\partial x_n}
\end{bmatrix}^\top,
$$</p><p>假设 $\mathbf{x}$ 为 $n$ 维向量，在微分多元函数时经常使用以下规则：</p><ul><li>对于所有 $\mathbf{A} \in \mathbb{R}^{n \times n}$，都有 $\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A} \mathbf{x} = \mathbf{A}^\top$</li><li>对于所有 $\mathbf{A} \in \mathbb{R}^{n \times m}$，都有 $\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A} = \mathbf{A}$</li><li>对于所有 $\mathbf{A} \in \mathbb{R}^{n \times n}$，都有 $\nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{A} \mathbf{x} = (\mathbf{A} + \mathbf{A}^\top) \mathbf{x}$</li><li>$\nabla_{\mathbf{x}} |\mathbf{x}|^2 = \nabla_{\mathbf{x}} \mathbf{x}^\top \mathbf{x} = 2 \mathbf{x}$</li></ul><p>同样，对于任意矩阵 $\mathbf{X}$，都有 $\nabla_{\mathbf{X}} |\mathbf{X}|_F^2 = 2 \mathbf{X}$。</p><h3 id=34-链式法则>3.4 链式法则</h3><p>考虑单变量函数 $y = f(u)$ 和 $u = g(x)$， 假设都是可微的，根据链式法则：
$$
\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}
$$</p><p>当函数具有任意数量的变量时，假设可微分函数 $y$ 有变量 $u_1, u_2, \cdots, u_m$，其中每个可微分函数 $u_i$ 都有变量 $x_1, x_2, \cdots, x_n$。注意，$y$ 是 $x_1, x_2, \cdots, x_n$ 的函数。对于任意 $i = 1, 2, \cdots, n$，链式法则给出：</p><p>$$
\frac{\partial y}{\partial x_i} = \frac{\partial y}{\partial u_1} \frac{\partial u_1}{\partial x_i} + \frac{\partial y}{\partial u_2} \frac{\partial u_2}{\partial x_i} + \cdots + \frac{\partial y}{\partial u_m} \frac{\partial u_m}{\partial x_i}.
$$</p><h2 id=4-自动微分>4. 自动微分</h2><p>Pytorch 使用自动微分 (automatic differentiation) 来加快求导。 实际中，根据设计好的模型，系统会构建一个计算图 (computational graph)，以跟踪计算是哪些数据通过哪些操作组合起来产生输出，并使用自动微分进行反向传播梯度。 这里，反向传播 (backpropagate) 意味着跟踪整个计算图，填充关于每个参数的偏导数。</p><p>当 $y$ 是标量时，可以通过链式法则反向求导输入参数的梯度，该梯度是一个与输入向量 $\bold{x}$ 形状相同的向量。</p><p>当 $y$ 不是标量时，向量 $\bold{y}$ 关于 $\bold{x}$ 的导数是一个矩阵，更高阶情况下是一个高阶张量。但当调用向量的反向传播计算时，通常会试图计算一批训练样本中每个组成部分的损失函数的导数。 这里、的目的不是计算微分矩阵，而是单独计算批量中每个样本的偏导数之和。如：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>arange(<span style=color:#ae81ff>4.0</span>)
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> x <span style=color:#f92672>*</span> x
</span></span><span style=display:flex><span>y<span style=color:#f92672>.</span>sum()<span style=color:#f92672>.</span>backward() <span style=color:#75715e># y.backward(torch.ones(len(x)))</span>
</span></span></code></pre></div><p>如果想将某些计算移到记录的计算图之外，例如，假设 $y$ 是作为 $x$ 的函数计算的，而 $z$ 则是作为 $y$ 和 $x$ 的函数计算的。比如，我们想计算 $z$ 关于 $x$ 的梯度，但由于某种原因，希望将 $y$ 视为一个常数，并且只考虑到 $x$ 在 $y$ 被计算后发挥的作用。</p><p>此时，可以使用 <code>detach()</code> 分离 $y$ 来返回一个新变量 $u$，该变量与 $y$ 具有相同的值，但丢弃计算图中如何计算 $y$ 的任何信息。换句话说，梯度不会向后流经 $u$ 到 $x$。因此，下面的反向传播函数计算 $z=u*x$ 关于 $x$ 的偏导数，$u$ 为常数。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x<span style=color:#f92672>.</span>grad<span style=color:#f92672>.</span>zero_()
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> x <span style=color:#f92672>*</span> x
</span></span><span style=display:flex><span>u <span style=color:#f92672>=</span> y<span style=color:#f92672>.</span>detach()
</span></span><span style=display:flex><span>z <span style=color:#f92672>=</span> u <span style=color:#f92672>*</span> x
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>z<span style=color:#f92672>.</span>sum()<span style=color:#f92672>.</span>backward()
</span></span></code></pre></div><h2 id=5-概率>5. 概率</h2><ul><li>单个随机变量：概率分布</li><li>多个随机变量<ul><li>联合概率 (联合分布)</li><li>条件概率 (条件分布)</li><li>贝叶斯定理</li><li>边际化 (边际概率、边际分布 &ndash; 全概率公式) $$P(B)=\sum_{A}P(A,B)$$</li><li>独立性</li><li>期望和方差</li></ul></li></ul></div><div class="row ps-3 pe-3"><div class="col-md-6 share-buttons"><strong>分享:</strong>
<a class="btn icon-button bg-facebook" href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fonline727.github.io%2fcn%2fposts%2fdltorch%2fch2%2f" target=_blank><i class="fab fa-facebook"></i>
</a><a class="btn icon-button bg-twitter" href="https://twitter.com/share?url=https%3a%2f%2fonline727.github.io%2fcn%2fposts%2fdltorch%2fch2%2f&text=%e7%ac%ac%e4%ba%8c%e7%ab%a0%20%e9%a2%84%e5%a4%87%e7%9f%a5%e8%af%86&via=%e8%b5%b5%e6%b5%a9%e7%bf%b0%20-%20%e5%8d%9a%e5%ae%a2" target=_blank><i class="fab fa-twitter"></i>
</a><a class="btn icon-button bg-reddit" href="https://reddit.com/submit?url=https%3a%2f%2fonline727.github.io%2fcn%2fposts%2fdltorch%2fch2%2f&title=%e7%ac%ac%e4%ba%8c%e7%ab%a0%20%e9%a2%84%e5%a4%87%e7%9f%a5%e8%af%86" target=_blank><i class="fab fa-reddit"></i>
</a><a class="btn icon-button bg-linkedin" href="https://www.linkedin.com/shareArticle?url=https%3a%2f%2fonline727.github.io%2fcn%2fposts%2fdltorch%2fch2%2f&title=%e7%ac%ac%e4%ba%8c%e7%ab%a0%20%e9%a2%84%e5%a4%87%e7%9f%a5%e8%af%86" target=_blank><i class="fab fa-linkedin"></i>
</a><a class="btn icon-button bg-whatsapp" href="https://api.whatsapp.com/send?text=%e7%ac%ac%e4%ba%8c%e7%ab%a0%20%e9%a2%84%e5%a4%87%e7%9f%a5%e8%af%86 https%3a%2f%2fonline727.github.io%2fcn%2fposts%2fdltorch%2fch2%2f" target=_blank><i class="fab fa-whatsapp"></i>
</a><a class="btn icon-button" href="mailto:?subject=%e7%ac%ac%e4%ba%8c%e7%ab%a0%20%e9%a2%84%e5%a4%87%e7%9f%a5%e8%af%86&body=https%3a%2f%2fonline727.github.io%2fcn%2fposts%2fdltorch%2fch2%2f" target=_blank><i class="fas fa-envelope-open-text"></i></a></div><div class="col-md-6 btn-improve-page"><a href=https://github.com/online727/online727.github.io/edit/main/content/posts/dltorch/ch2/index.cn.md title=改善此页面 target=_blank rel=noopener><i class="fas fa-code-branch"></i>
改善此页面</a></div></div><hr><div class="row next-prev-navigator"><div class="col-md-6 previous-article"><a href=/cn/posts/quant/multi-factors/perf-attri/ title=多因子绩效归因 class="btn filled-button"><div><i class="fas fa-chevron-circle-left"></i> 上一篇</div><div class=next-prev-text>多因子绩效归因</div></a></div><div class="col-md-6 next-article"><a href=/cn/posts/dltorch/ch3/ title="第三章 线性神经网络" class="btn filled-button"><div>下一篇 <i class="fas fa-chevron-circle-right"></i></div><div class=next-prev-text>第三章 线性神经网络</div></a></div></div><hr></div></div></div><a id=scroll-to-top class=btn type=button data-bs-toggle=tooltip data-bs-placement=left title="Scroll to top"><i class="fas fa-chevron-circle-up"></i></a></section><section class=toc-section id=toc-section><div class=toc-holder><h5 class="text-center ps-3">目录</h5><hr><div class=toc><nav id=TableOfContents><ul><li><a href=#1-tensor-操作和数据预处理>1. Tensor 操作和数据预处理</a></li><li><a href=#2-线性代数>2. 线性代数</a></li><li><a href=#3-微积分>3. 微积分</a><ul><li><a href=#31-导数和微分>3.1 导数和微分</a></li><li><a href=#32-偏导数>3.2 偏导数</a></li><li><a href=#33-梯度>3.3 梯度</a></li><li><a href=#34-链式法则>3.4 链式法则</a></li></ul></li><li><a href=#4-自动微分>4. 自动微分</a></li><li><a href=#5-概率>5. 概率</a></li></ul></nav></div></div></section></div><footer id=footer class="container-fluid text-center align-content-center footer pb-2"><div class="container pt-5"><div class="row text-start"><div class="col-md-4 col-sm-12"><h5>导航</h5><ul><li class=nav-item><a class=smooth-scroll href=https://online727.github.io/cn/#about>关于</a></li><li class=nav-item><a class=smooth-scroll href=https://online727.github.io/cn/#education>教育</a></li><li class=nav-item><a class=smooth-scroll href=https://online727.github.io/cn/#experiences>经历</a></li><li class=nav-item><a class=smooth-scroll href=https://online727.github.io/cn/#projects>项目</a></li><li class=nav-item><a class=smooth-scroll href=https://online727.github.io/cn/#skills>技能</a></li><li class=nav-item><a class=smooth-scroll href=https://online727.github.io/cn/#recent-posts>近期帖子</a></li></ul></div><div class="col-md-4 col-sm-12"><h5>联系方式:</h5><ul><li><a href=mailto:zhhohoh27@gamil.com target=_blank rel=noopener><span><i class="fas fa-envelope"></i></span> <span>zhhohoh27@gamil.com</span></a></li><li><a href=https://github.com/online727 target=_blank rel=noopener><span><i class="fab fa-github"></i></span> <span>online727</span></a></li><li><a href=https://www.linkedin.com/in/haohan-zhao-9a6b24296 target=_blank rel=noopener><span><i class="fab fa-linkedin"></i></span> <span>赵浩翰</span></a></li><li><span><i class="fas fa-phone-alt"></i></span> <span>+8619551998168</span></li></ul></div></div></div><hr><div class=container><p id=disclaimer><strong>免责声明:</strong> 本网站基于 hugo 和 github pages 搭建，使用 hugo-toha 主题。 网站用于个人博客，所有内容均为本人所有，且不构成任何相关建议，如有问题，可与本人联系。</p></div><hr><div class=container><div class="row text-start"><div class=col-md-4><a id=theme href=https://github.com/hugo-toha/toha target=_blank rel=noopener><img src=/images/theme-logo_hu16779671404603505019.png alt="Toha Theme Logo">
Toha</a></div><div class="col-md-4 text-center">© 2020 版权.</div><div class="col-md-4 text-end"><a id=hugo href=https://gohugo.io/ target=_blank rel=noopener>Powered by
<img src=/images/hugo-logo.svg alt="Hugo Logo" height=18></a></div></div></div></footer><script src=/application.aa1b29568827d862abc07cd2a25fa9f3f1dad96fd8fe2a012bccae2de39367fc.js integrity="sha256-qhspVogn2GKrwHzSol+p8/Ha2W/Y/ioBK8yuLeOTZ/w=" defer></script></body></html>