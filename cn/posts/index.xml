<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>博文 on 赵浩翰 - 博客</title><link>https://online727.github.io/cn/posts/</link><description>Recent content in 博文 on 赵浩翰 - 博客</description><generator>Hugo -- gohugo.io</generator><language>cn</language><lastBuildDate>Sun, 28 Jul 2024 21:57:00 +0800</lastBuildDate><atom:link href="https://online727.github.io/cn/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>第三章 线性神经网络</title><link>https://online727.github.io/cn/posts/dltorch/ch3/</link><pubDate>Sun, 28 Jul 2024 21:57:00 +0800</pubDate><guid>https://online727.github.io/cn/posts/dltorch/ch3/</guid><description>1. 线性回归 1.1 线性回归的基本元素 1.1.1 线性模型 线性回归，假设自变量 $\bold{x}$ 和因变量 $y$ 之间为线性关系，其中可能包含噪声，但噪声是比较正常的，如噪声服从正态分布。
给定一个样本 $\bold{x}\in\mathbb{R}^{d}$，即具有 $d$ 个特征，将所有系数记为 $\bold{w}\in\mathbb{R}^{d}$，线性回归的基本形式为： $$ \hat{y} = \bold{w}^{T}\bold{x} + b $$
矩阵形式下，$\bold{X}\in\mathbb{R}^{n\times d}$ 为所有样本的特征，此时线性回归表示为： $$ \hat{\bold{y}} = \bold{Xw} + b $$
给定训练数据集 $\bold{X}$ 和对应标签 $\bold{y}$，线性回归的目标就是找到一组权重向量 $\bold{w}$ 和偏置 $b$，使得所有样本的预测误差尽可能小。
1.1.2 损失函数 损失函数，用以度量上面提到的 “预测误差”，通常选择一个非负数作为损失，且该损失越小越好。回归问题中，最常用的损失函数为 平方误差，当样本 $i$ 的预测值为 $\hat{y}^{(i)}$，相应真实标签为 $y^{(i)}$ 时，平方误差定义为： $$ l^{(i)}(\bold{w}, b) = \frac{1}{2}\left(\hat{y}^{(i)} - y^{(i)} \right)^{2} $$
$\frac{1}{2}$ 是为了损失函数求导时常数系数为 1,不会有本质差别。
那么，为了度量模型在整个训练集上的表现，就需要计算在整个训练集 $n$ 个样本上的损失均值 (等价于求和)： $$ L(\bold{w}, b) = \frac{1}{n}\sum_{i=1}^{n}l^{(i)}(\bold{w},b) = \frac{1}{n}\sum_{i=1}^{n}\frac{1}{2}\left(\bold{w}^{T}\bold{x}^{(i)} + b - y^{(i)} \right)^{2} $$</description></item><item><title>第二章 预备知识</title><link>https://online727.github.io/cn/posts/dltorch/ch2/</link><pubDate>Sat, 27 Jul 2024 17:06:25 +0800</pubDate><guid>https://online727.github.io/cn/posts/dltorch/ch2/</guid><description>接下来一段时间，我想自学深度学习，使用的教材为 动手学深度学习 (Pytorch 版)，该书有线上网址，且提供配套代码和相关 Python 包，详情可参见 动手学深度学习。
第一章内容，介绍了深度学习的相关背景和应用场景，以及深度学习领域常见的术语和名词，有一定机器学习经验的人或许已比较熟悉，故不再赘述，我们直接从第二章开始。
1. Tensor 操作和数据预处理 深度学习中的数据以张量 (tensor) 形式存储，支持 GPU 计算和 autograd 自动微分。
张量的创建、变形、运算 (按元素 / 矩阵)、广播机制、索引、切片等均与 numpy.ndarray 类似。
节省内存： Y = X + Y 不是原地操作，即：id(Y = X + Y) != id(Y)，会分配新的内存。 使用 Y[:] = X + Y 或 Y += X 进行原地操作以避免不必要的内存分配。 Tensor 可以与其他 Python 对象互相转换，如 tensor.numpy()。大小为 1 的张量可以转化为 Python 标量，使用 tensor.item() 或 float(tensor) 等。
数据需要经过预处理，如填充 nan，标准化等，可以借用其他 Python 包处理后再转化为 tensor。
2. 线性代数 标量，以小写字母 $x,y,z$ 等表示。 向量，以粗体小写字母 $\bold{x,y,z}$ 表示，向量的维度 (形状) 代表元素个数 (向量长度)，可以使用 len(x), x.</description></item><item><title>多因子绩效归因</title><link>https://online727.github.io/cn/posts/quant/multi-factors/perf-attri/</link><pubDate>Tue, 23 Jul 2024 23:36:25 +0800</pubDate><guid>https://online727.github.io/cn/posts/quant/multi-factors/perf-attri/</guid><description>1. 摘要 投资组合的业绩归因可以分为 收益归因 和 风险归因 两个部分，而归因又可以基于净值或持仓进行。本文主要基于 持仓数据 对组合的业绩归因进行探讨。
在组合收益归因方面，主要有以下部分：
基于 Brinson 模型 经典版 BHB (Brinson, Hood and Beebower) 模型：将组合超额收益分解为配置收益、选股收益和交互收益 3 部分。 改进版 BF (Brinson and Fachler) 模型：引入行业超额收益，将组合超额收益分解为配置效应和选股效应两个部分。 基于多因子模型 基于行业的多因子收益归因：与自下而上的 Brinson 模型完全一致。 基于行业和风格的多因子收益归因：同时对行业和风格上的配置进行分析。 在组合风险归因方面，主要基于多因子模型：
单一波动分解法：单独考虑每个因子，计算简单，但忽略因子之间的协同影响，且不具可加性。 边际风险分解法：将组合风险分解为因子暴露度与因子边际风险贡献的乘积，然而偏导数的概念相对模糊，指导意义不强。 三要素分解法：将风险分解为因子暴露 ($x$)、因子波动 ($\sigma$) 和因子-组合相关系数 ($\rho$)，对风险的分解更为透彻，更有利于投资经理对风险进行控制。 2. 基于 Brinson 模型的组合收益归因 2.1 经典 BHB 模型 BHB 模型将投资组合的超额收益率分解为 配置收益、选股收益和交互收益 三个部分，其基本框架如下图所示，其中红色渲染部分表示投资组合的超额收益。
从行业配置的角度而言，假设 $w_{i}^{P}, w_{i}^{B}$ 分别表示投资组合和基准组合中行业 $i$ 的权重，$r_{i}^{P}, r_{i}^{B}$ 分别表示投资组合和基准组合中行业 $i$ 的收益率，那么投资组合的收益率 $R^{P}$ 和基准组合的收益率 $R^{B}$ 就可以表示为： $$ \begin{align*} R^{P}&amp;amp;=\sum_{i=1}^{I}w_{i}^{P}r_{i}^{P}, where\sum_{i=1}^{I}w_{i}^{P}=1 \cr R^{B}&amp;amp;=\sum_{i=1}^{I}w_{i}^{B}r_{i}^{B}, where\sum_{i=1}^{I}w_{i}^{B}=1 \end{align*} $$</description></item></channel></rss>